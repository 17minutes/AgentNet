{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=gpu2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "#the problem is too simple to be run on GPU. Seriously.\n",
    "%env THEANO_FLAGS='device=gpu2'\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "floatX = theano.config.floatX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [in development] this is just a minimalistic language model that uses stack-augmented memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sequence(batch_size = 10,crop_length = 50 ):\n",
    "    \"\"\"\n",
    "    Generates sequence from pattern [0*n 1*m 2*(n+m)]\n",
    "    \"\"\"\n",
    "    sequences=[]\n",
    "    for i in range(batch_size):\n",
    "        seq = []\n",
    "        while len(seq) < crop_length:\n",
    "            n,m = np.random.randint(1,5,2)\n",
    "        \n",
    "            seq += [0] + [1]*n+[2]*m+[3]*(n+m)\n",
    "        seq = seq[:crop_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences,dtype='int32')\n",
    "\n",
    "alphabet = np.array(list('|abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 4.58 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, ..., 2, 2, 2],\n",
       "       [0, 1, 2, ..., 1, 1, 1],\n",
       "       [0, 1, 1, ..., 3, 3, 3],\n",
       "       ..., \n",
       "       [0, 1, 1, ..., 2, 2, 3],\n",
       "       [0, 1, 2, ..., 0, 1, 2],\n",
       "       [0, 1, 1, ..., 2, 3, 3]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "generate_sequence(100,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation has to contain three parts:\n",
    " * Memory layer(s)\n",
    "  * in this case, a single one-step GRU\n",
    " * Q-values evaluation layers\n",
    "  * in this case, a lasagne dense layer based on memory layer\n",
    " * Resolver - acton picker layer\n",
    "  * in this case, the resolver has epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from agentnet.memory import GRUMemoryLayer\n",
    "from agentnet.agent import Generator\n",
    "from agentnet.resolver import ProbablisticResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sequence = T.matrix('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#observation\n",
    "output_shape = (None,)\n",
    "observation_layer = lasagne.layers.InputLayer(output_shape,name=\"obs_input\")\n",
    "\n",
    "n_tokens = len(alphabet)\n",
    "\n",
    "def to_one_hot(x):\n",
    "    return T.extra_ops.to_one_hot(x,n_tokens,dtype=floatX)\n",
    "\n",
    "obs_one_hot = lasagne.layers.ExpressionLayer(observation_layer,to_one_hot,\n",
    "                                     output_shape=output_shape+(n_tokens,),\n",
    "                                     name=\"token_one_hot\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#memory\n",
    "\n",
    "n_hid_1 = 256\n",
    "prev_gru1_layer = lasagne.layers.InputLayer((None,n_hid_1),name=\"prev_gru1_state_input\")\n",
    "\n",
    "gru1 = GRUMemoryLayer(n_hid_1,\n",
    "                     obs_one_hot,\n",
    "                     prev_gru1_layer,\n",
    "                     name=\"gru1\")\n",
    "\n",
    "\n",
    "\n",
    "n_hid_2 = 256\n",
    "\n",
    "prev_gru2_layer = lasagne.layers.InputLayer((None,n_hid_2),name=\"prev_gru2_state_input\")\n",
    "\n",
    "gru2 = GRUMemoryLayer(n_hid_2,\n",
    "                     gru1,        #note that it takes CURRENT gru1 output as input.\n",
    "                                  #replacing that with _prev_gru1_state would imply taking previous one.\n",
    "                     prev_gru2_layer,\n",
    "                     name=\"gru2\")\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "memory_dict = OrderedDict([\n",
    "            (gru1,prev_gru1_layer),\n",
    "            (gru2,prev_gru2_layer)\n",
    "    ])\n",
    "\n",
    "\n",
    "#policy\n",
    "\n",
    "greed = theano.shared(np.float32(1),\"prob_multiplier\")\n",
    "\n",
    "\n",
    "policy_layer = lasagne.layers.DenseLayer(gru2, #taking both memories. \n",
    "                                                        #Replacing with gru1 or gru2 would mean taking one\n",
    "                                         num_units = n_tokens,\n",
    "                                         nonlinearity=lambda x: lasagne.nonlinearities.softmax(x*greed),\n",
    "                                         name=\"policy_original\")\n",
    "\n",
    "#resolver\n",
    "\n",
    "\n",
    "resolver = ProbablisticResolver(policy_layer,assume_normalized=True,name=\"resolver\")\n",
    "\n",
    "assert tuple(lasagne.layers.get_output_shape(resolver)) == tuple(output_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Generator(\n",
    "    observation_layer,\n",
    "    memory_dict,\n",
    "    policy_layer,\n",
    "    resolver\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gru1.W_in_to_updategate,\n",
       " gru1.W_hid_to_updategate,\n",
       " gru1.b_updategate,\n",
       " gru1.W_in_to_resetgate,\n",
       " gru1.W_hid_to_resetgate,\n",
       " gru1.b_resetgate,\n",
       " gru1.W_in_to_hidden_update,\n",
       " gru1.W_hid_to_hidden_update,\n",
       " gru1.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " policy_original.W,\n",
       " policy_original.b]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * One-step recurrent layer\n",
    "     * takes input and one's previous state\n",
    "     * returns new memory state\n",
    "   * Can be arbitrary lasagne layer\n",
    "   * Several one-step recurrent units are implemented in __agentnet.memory__\n",
    "   * Note that lasagne's default recurrent networks roll for several steps at once\n",
    "     * in other words, __using lasagne recurrent units as memory means recurrence inside recurrence__\n",
    " * Using more than one memory layer is explained in farther tutorials\n",
    "\n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are represented as tensors with dimensions matching pattern [batch_session_i, time_tick, ...]\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "\n",
    "sequences_batch = theano.shared(np.zeros([3,seq_length],dtype=\"int32\"),name=\"reference_sequences\")\n",
    "\n",
    "batch_size = sequences_batch.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = agent.get_sessions(session_length=seq_length,\n",
    "                             recorded_sequences=sequences_batch,\n",
    "                             batch_size=batch_size,)\n",
    "\n",
    "env_states,observation_seq,agent_states,action_seq,policy_seq = history\n",
    "\n",
    "\n",
    "gru1_seq = agent_states[gru1]\n",
    "gru2_seq = hidden_seq = agent_states[gru2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this case, we want to \n",
    " * first get pairs of (predicted Qvalue, reference Qvalue) for all actions commited\n",
    " * second, define loss function\n",
    " * third, compute grad and update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_probas = policy_seq[:,:-1].reshape([-1,n_tokens])\n",
    "predicted_probas = T.maximum(predicted_probas,1e-10)\n",
    "\n",
    "model_loss = lasagne.objectives.categorical_crossentropy(predicted_probas,\n",
    "                                                         sequences_batch[:,1:].ravel()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = model_loss + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#actions generated in active mode\n",
    "generated_action_seq = agent.get_sessions(session_length=seq_length,\n",
    "                             recorded_sequences=sequences_batch,\n",
    "                             batch_size=batch_size,)[-2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss],updates=updates)\n",
    "evaluation_fun = theano.function([],[loss,model_loss,reg_l2])\n",
    "get_sequences = theano.function([],generated_action_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0\tfull:0.46341\tllh:0.40408\treg:0.05933\n",
      "a b c c a b c c c c c c c | c | | a a b a a b b c c c c c c c | | a b b c b b c c c c c c b a b c c\n",
      "a b a b b c b b c c c c c c c | | a a a a a b c c c c c c c a a a b c c b c | a a b a b b c c c c c\n",
      "a a b c b b c c | a a c b c c | a a a b b b c c c c c c c a a a b b c c c c c | a a b a c b c c c c\n",
      "iter:100\tfull:0.43279\tllh:0.37342\treg:0.05937\n",
      "b a b b b b b b c c c c c c | a a a b b c c | c | a a a b b c b b c c c c c c | a a a a b b c c c b\n",
      "b a a b b b c c c c | | a a b b b c c c c c c c c | a a a b b b b c b c c c c c c c | a a b b c c |\n",
      "a a a a b b c c c c c c c c | a a b b c b c c c c | a a b b b c c a a b b c c c c c | a b b b c c c\n",
      "iter:200\tfull:0.37810\tllh:0.31870\treg:0.05940\n",
      "a a b b c c c c c c | a a a b b b c c c c | a b a c c c | a a b b c b c c c c c c c a a b b b b c c\n",
      "a a a b b b c c c c | a a b a b c b c c c c a b b b c c c c c c c c | a a b c c a a b b c c c c c c\n",
      "a a a b b b c c c | | a a a b b b b c c c c c c c c c | a a a b a b b b c c c c c c c | | a a b b c\n",
      "iter:300\tfull:0.38419\tllh:0.32477\treg:0.05943\n",
      "| a a a c b c c c c | a a a b a b c b c c c c c c c a | a a a a b b c c c | | a a a b b c c c | a a\n",
      "| a a b b c c c | a a b c | a a b b c b c c c c | a a a b b b b c c c c c | a a a a b b b c c c c c\n",
      "| a a b c c | a a a a b b c c c c | a a a a a b c c c | | a a a b b c c c | a a a b b b c c c c | a\n",
      "iter:400\tfull:0.35813\tllh:0.29869\treg:0.05944\n",
      "a b a c b b c c c c c c | a a a b b b b c c c c c c c c | | a a a a b b b c c c c c c c | a a b b c\n",
      "a a c c c a a b b c c | a a a c b b b c c c c c c c | a b b b b c c c | b b b b c c | a a b b b c c\n",
      "a b a a b b c b b c c c c c c c | a a a b b b b c c c c c c c c c | a b c b c c c c | a a b a b b c\n",
      "iter:500\tfull:0.35830\tllh:0.29884\treg:0.05945\n",
      "a a b b c c | a b b a b b c c c c | a a a b c b c c c c | a b a a c c b c | a a b c c c c c | a a b\n",
      "b a b c | a a b b b b b c c c c c c c c | c a a c c c a a a b c c c c c c | a a a c b b c c c c | a\n",
      "a b b b c c c c | a a b b b b b c c c c c | | a a b b b b c c c c c c c c c | a a b b c c c c | a a\n",
      "iter:600\tfull:0.34082\tllh:0.28137\treg:0.05945\n",
      "a a b b b b c c c c | a a b a b b c c c c | a a b b c c c c c c | a b a c b c c c c c c c c a b b b\n",
      "a a a b b b b b c c c c c c | a a b b c c c | | a b a c b c | a b b c | a a a b c b c c c c | a a b\n",
      "a a b c c c c c c c | a b a b b b c c c c c c c c | a a b a b c c c c a a a b c b c c c c c c c c |\n",
      "iter:700\tfull:0.35071\tllh:0.29126\treg:0.05946\n",
      "a b b c c c | a a b b c c c c | a a a a b b c c c c c c c c | a a a b b c c c | a a a a b c c c | a\n",
      "a a a a b c c c c c c c | a a c b c c | a a b b b b c c | a a a a b b c c c c c c c c | a a b c c c\n",
      "a a b b c b c c c c c c | a a a a b c c c c c c c c c | a a a c c c | a a b b b c c c | a b b b c c\n",
      "iter:800\tfull:0.33526\tllh:0.27581\treg:0.05945\n",
      "a b a a b b c c c c | a a a a b b c c c c c c c c | a a b b c b c c c c | a b a b b b c c c c | a a\n",
      "a a a b b b c c c c c c c c | a a a c b c c c | a a a c b c c c | a a c c | a b a b c b c c c c | a\n",
      "a a b a b b b c c c c c c c | a a b b b b c c c c c c c c | a b b b c c a a a b c b c c c c c | a a\n",
      "iter:900\tfull:0.33729\tllh:0.27785\treg:0.05945\n",
      "a a b c c c | a a a b b b c c c c c c c c | a b a b b c c c | a a b b c c c c | a a b a b b b b c c\n",
      "a a b b b b c c c c | a a b b b b b c c c c c c c | a a b b c b c c c c c c | a b a b b b c c c c |\n",
      "a a b c c c | a a b b b b c c c c c c | a b a a b b c c c c | a a a b b c b c c c c c | a b b b b b\n",
      "iter:1000\tfull:0.33685\tllh:0.27741\treg:0.05944\n",
      "a a b b c c | a a a a b b c c c c c c | a a c c b c c c | a b b c | a a b b c c | a b a a b c c c c\n",
      "c a b b c c | a a a a b c c c c c | a a a c c c | a a a a b b c b c c c c c c | a a a a a c c c c c\n",
      "a a a a b b c c c c | a a b b b c b b c c c c c c c c | a a b b b c c c c c | a a b b b b c c c c |\n",
      "iter:1100\tfull:0.32432\tllh:0.26489\treg:0.05943\n",
      "a a b a b b c c c c | a b b b b b c c c c | a a b b c c c c | a a a a b c c c c c | a b a b c c | a\n",
      "a a a a b c c c c c | a a a b b c c c c c c c c c | a a a a b b b b c c c c c c c c | a a a a b c c\n",
      "a a a b b b c c c c c c | a a a b b b c c c c | a a b a b b c c c c c c | a a a b c c | a a a a a b\n",
      "iter:1200\tfull:0.33082\tllh:0.27140\treg:0.05942\n",
      "a a a a b c c c | a b b b c c | a a a b b b b c c c c c | a a b b c c | a a b b c b c c c c | a b b\n",
      "a a b b c b c c c c | a a b b c c | a a a b a b b b c c c c c c c c | a a a b b b c c c c | a a b b\n",
      "a a a a b c c c c c | a a a b c c c c | a b a b b b c c c c | a b b a b b b b c c c c c c | a a b b\n",
      "iter:1300\tfull:0.33463\tllh:0.27522\treg:0.05941\n",
      "a a a b c c c c c c | a b a b b b c b c c c c c c | a a a b b b c c c c c c | a b a b b c c c c c c\n",
      "a a a a b c c c | a a a b c c | a b b c | a b a a c c c c c c c c c c | a b a b b b b c c c c c | a\n",
      "a a a a b b b b c c c c c c | a b a b c c c c c a a c b c c | a a a b b b b c c c c c | a a b b b b\n",
      "iter:1400\tfull:0.34582\tllh:0.28643\treg:0.05939\n",
      "a a b c | a a b a b b b b c c c c c c | a b b b b b b b c c c c c c c c | a b b b b b c c c c c c |\n",
      "a a a c b b c c c c | a a b b b c c c c c | a a c c | a a a b b b c c c c | a b a b b c c c c c c c\n",
      "a a a b b c c c | a a b b b b c c c c c c | a b a b c c | a b a b c c | a b a b b b c c c c | a a b\n",
      "iter:1500\tfull:0.32453\tllh:0.26515\treg:0.05938\n",
      "a a a b b c c c c c c c | a b b b b c c c | a b a b b b b b c c c c c c c c | a a b b b b c c c c |\n",
      "a b a b c c | a a b a b b b c c c c c c c | a a b b b b b c c c c c | a a a a b b b b c c c c c c c\n",
      "a a b b b b c c c c | a a a a b c c c c c | a a b a b c c c c c c c c c | a b a b b c c c c c | a a\n",
      "iter:1600\tfull:0.32552\tllh:0.26615\treg:0.05936\n",
      "a a b b a b b b c c c c c c c c | a b a b b b c c c c c c c c | a b b b b c c c c c | a a b b c c c\n",
      "a a a b c c | a a a c b c c c | a a b a b c b c c c c c | a a b b b b c b c c c c c c | a a b a b b\n",
      "a b a b a b c c c c | a a a b c c | a a a b b b c c c c | a b b a b b b b c c c c c c | a a b b b c\n",
      "iter:1700\tfull:0.32153\tllh:0.26218\treg:0.05935\n",
      "a a a b b c b c c c c c | a b c b c b c c c c | a b b b c c c c c c | a a b b c c | a a a b b c b c\n",
      "b a a b b b b c c c c c | a a b b c b c c c c | a b b b b b c c c c c c | a a c c | a a a b b b b c\n",
      "a a a b b b b c c c c c c c | a a a b b c c c c c | a a b c | a b b c b b c c c c | a a b b b b c c\n",
      "iter:1800\tfull:0.35243\tllh:0.29310\treg:0.05933\n",
      "a a a b c c c c | a a b a b b c c c c | a b a b c c c c | a a b b b c c c | a a b b c b c c c c | a\n",
      "a a b c | a a b c | a a a b b b c c c c c c c c c c | a b a a b c b c c c c c c c c c | a a a a b c\n",
      "a a a a b c c c c c | a a c b b c c c | a a b b c c | a b a a b b c c c c | a a a a b c b c c c c c\n",
      "iter:1900\tfull:0.33905\tllh:0.27973\treg:0.05932\n",
      "a b a b c c c c c c | a a a b b b b c c c c c c c c c | a a b c c c c c | | a b a b b c c c c | | a\n",
      "b a a b b c b b c c c c c c | a b b a b b b c c c c c | a a b a c b b c c c c c | a b b b c b c c c\n",
      "a a b c | a a a b b c c c | a a b a b b b b c c c c c c | a b b a b b b c c c c c c c | a a a a b c\n",
      "iter:2000\tfull:0.32179\tllh:0.26249\treg:0.05930\n",
      "a a b b b b b c c c c c | a b a a b c c c c c | a b a c b c c c c c | a b b b b c c c | a a b a c b\n",
      "a a a b b b c c c c | a a a c b c c c c c | a a a b c c c c | a a a b b c c c | a a b c a a b b b b\n",
      "a a b a b b c c c c | a a a b b c c c c c c c c c | a b a b b c b b c c c c c c c c | a a b b b c c\n",
      "iter:2100\tfull:0.33474\tllh:0.27545\treg:0.05929\n",
      "a a b a c b c b c c c c c c | a a b b c c | a a a b b c c c c c | a b b a c c b c c c c c | a b a b\n",
      "a a b b b c b c c c c c c c | a b a b b b c c c c c c c c | a a b b c c | a a a b b b b c c c c c c\n",
      "a a b b b b c c c c | a a b b b b c c c c | a a b b c c c c c c | a a a b b c c c | a a b b b b c c\n",
      "iter:2200\tfull:0.30861\tllh:0.24934\treg:0.05927\n",
      "a a a a b b c b c c c c c c | a a a c c b c c c c c c | a a a a b c b c c c c c c c | a a a a b b c\n",
      "a a a b b c c c c c c c | a a a b b b b c c c c c c c | a a a b b c b c c c c c | a a b b b c c c |\n",
      "a b a a b c c c c c c c | a a b c c b c c c c | a b b c | a a c b b c c c | a a a a b b c c c c | a\n",
      "iter:2300\tfull:0.31627\tllh:0.25702\treg:0.05925\n",
      "a a a b c c c c | a a a a b b b b c c c c c c | a a a b b c c c | a a a a c c c c c c c c c c | a a\n",
      "a a a b b b c c c c c c | a a c b b c c c c c | a b a b b b c b c c c c c c c c | a a a b c b c c c\n",
      "a a b b b c c c | a a a b b b b b c c c c c c c c | a a b b c c | a a a c b c c c | a a c c | a b c\n"
     ]
    }
   ],
   "source": [
    "loss_seq = []\n",
    "for i in range(50000):\n",
    "    new_batch = generate_sequence(10,seq_length)\n",
    "    sequences_batch.set_value(new_batch)\n",
    "    \n",
    "    loss_seq.append(train_fun())\n",
    "    \n",
    "    if i % 100==0:\n",
    "        quality = \"iter:%i\\tfull:%.5f\\tllh:%.5f\\treg:%.5f\"%tuple([i]+map(float,evaluation_fun()))        \n",
    "        print quality\n",
    "        log+=quality+'\\n'\n",
    "        \n",
    "        examples = get_sequences()[:3]\n",
    "        for tid_line in examples:\n",
    "            line = ' '.join(map(alphabet.__getitem__,tid_line))\n",
    "            print line\n",
    "            log += line+'\\n'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print log[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save\n",
    "save(resolver,\"./lm-dense.pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
