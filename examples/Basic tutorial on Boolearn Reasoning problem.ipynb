{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "#the problem is too simple to be run on GPU. Seriously.\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial explains the basic pipline of Agentnet experiment\n",
    "* experiment setup\n",
    "* designing agent\n",
    "* interacting with environment\n",
    "* computing losses\n",
    "* training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we load a simple experiment environment (description below)\n",
    "* Designing one from scratch is explained in later tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import agentnet.experiments.boolean_reasoning as experiment\n",
    "print(experiment.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "env = experiment.BooleanReasoningEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation contains three parts:\n",
    " * Memory layer(s)\n",
    "   * in this case, a single one-step RNN\n",
    "   * may be any amount of recurrent layers [GRU/LSTM, LTM, custom or none at all]\n",
    "   * you have to create a dict {new recurrent state : previous state} (see next tabs)\n",
    "   \n",
    " * Policy estimation layers\n",
    "   * In this case, predicted Qvalues for all actions (via DenseLayer)\n",
    "   * Whatever is required for agent to pick action\n",
    "   * Can be any lasagne network\n",
    "   \n",
    " * Resolver - acton picker\n",
    "   * in this case, the resolver has epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.memory.rnn import RNNCell\n",
    "from agentnet.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer\n",
    "\n",
    "#64 hidden neurons\n",
    "n_hid=64\n",
    "\n",
    "\n",
    "observation_size = (None,)+tuple(env.observation_shapes)\n",
    "\n",
    "observation_layer = InputLayer(observation_size,name=\"obs_input\")\n",
    "prev_state_layer = InputLayer([None,n_hid],name=\"prev_state_input\")\n",
    "\n",
    "#memory\n",
    "#note that this isn't the same as lasagne recurrent units (see next text tab for detail)\n",
    "rnn = RNNCell(prev_state_layer,\n",
    "              observation_layer,\n",
    "              name=\"rnn0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Qvalues estimator\n",
    "q_eval = DenseLayer(rnn,\n",
    "                    num_units = env.n_actions,\n",
    "                    nonlinearity=lasagne.nonlinearities.linear,name=\"QEvaluator\")\n",
    "\n",
    "\n",
    "\n",
    "#create epsilon-greedy resolver with default epsilon [theano shared]\n",
    "resolver = EpsilonGreedyResolver(q_eval,name=\"resolver\")\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              {rnn:prev_state_layer},\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about memory layers\n",
    "\n",
    "In AgentNet, recurrent layers are defined as one-step layers that return new state given last state and inputs.\n",
    "\n",
    "Using basic lasagne recurrences is still okay, but one should understand what they mean.\n",
    "\n",
    "To create exactly what is above using lasagne layers only, one should use\n",
    "```\n",
    "from lasagne.layers import dimshuffle, RecurrentLayer\n",
    "\n",
    "#reshape observation as 1-element sequence\n",
    "observation_reshape = dimshuffle(observation_layer,(0,'x',1))\n",
    "\n",
    "rnn = RecurrentLayer(observation_reshape,       #observation input\n",
    "                     num_units=n_hid,           # amount of cells\n",
    "                     hid_init=prev_state_layer,  #initialize with previous state\n",
    "                     only_return_final=True,   #return final state, not sequence\n",
    "                     unroll_scan=True,      #highly recommended for speedup\n",
    "                     name='rnn')\n",
    "```\n",
    "\n",
    "\n",
    "Lasagne recurrence is also very useful to create recurrence inside recurrence.\n",
    "\n",
    "Say, you have a text sequence as input on each step (e.g. conversation models).\n",
    "Than you can read it with any recurrent layer and embed the result into your agent's state using other lasagne layers.\n",
    "\n",
    "\n",
    "Alternatively, one can build RNN layer as an ElemwiseSumLayer of two DenseLayers without nonlinearity for input and prev state (and than apply NonlinearityLayer to the sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are represented as tensors with dimensions matching pattern [batch_session_i, time_tick, ...]\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#produce interaction sequences of length <= 10\n",
    "(state_seq,),observation_seq,agent_state,action_seq,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=10,\n",
    "    batch_size=env.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "hidden_seq = agent_state[rnn]\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.get_reward_sequences(state_seq,action_seq)\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.get_whether_alive(observation_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "Here we use a simple Q-learning algorithm.\n",
    "\n",
    "The function below \n",
    "* takes qvalues, actions, rewards and session indicators,\n",
    "* computes reference Q-values as $Qref(S,a_{taken}) = r + \\gamma \\cdot \\max _{a} (Q(S_{next},a))$\n",
    "* returns elementwise MSE, $L = ( Qpref(S,a_{taken}) - Qref(S,a_{taken}))^2$\n",
    "\n",
    "\n",
    "AgentNet has plenty of such algorithms \n",
    "* n-step Qlearning, SARSA, actor-critic, det. policy gradient, etc\n",
    "* one can easily define (and contribute) their other algorithms by theano operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#compute MSE between reference Qvalues and predicted ones\n",
    "#use default gamma\n",
    "\n",
    "squarred_Qerror = qlearning.get_elementwise_objective(\n",
    "    qvalues_seq,\n",
    "    action_seq,\n",
    "    rewards_seq,\n",
    "    is_alive_seq,\n",
    "    gamma_or_gammas = 0.9)\n",
    "\n",
    "\n",
    "loss = squarred_Qerror.sum(axis = 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### expected total reward per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "batch_size=10\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    #train\n",
    "    env.generate_new_data_batch(batch_size)\n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon =  0.05 + 0.95*np.exp(-epoch_counter/2500.)\n",
    "    resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##show current learning progress\n",
    "    if epoch_counter%100 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected epsilon-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "\n",
    "        print(\"epoch %i, mse %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy))\n",
    "\n",
    "        \n",
    "    #visualize learning curve and sample sessions\n",
    "    if epoch_counter %1000 ==0:\n",
    "        print(\"Learning curves:\")\n",
    "        score_log.plot()\n",
    "        \n",
    "    epoch_counter  +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session visualization tools\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* usually useful to get insight on what worked and what din't\n",
    "* in this case, we print strings following pattern\n",
    "  * [action_name] ([predicted action qvalue]) -> reward [reference qvalue] | next iteration\n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are Qvalues for actions\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "__Warning! the visualization tools are underdeveloped and only allow simple operations.__\n",
    "\n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        hidden_seq,qvalues_seq, action_seq,rewards_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False):\n",
    "        \n",
    "    hidden_log,qvalues_log,actions_log,reward_log, is_alive_log = get_printables()\n",
    "    \n",
    "    \n",
    "    print_sessions(qvalues_log,actions_log,reward_log,\n",
    "                   is_alive_seq = is_alive_log,\n",
    "                   action_names=env.feature_names,\n",
    "                  \n",
    "                  plot_policy = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resolver.epsilon.set_value(np.float32(0.))\n",
    "print(\"Random session examples\")\n",
    "env.generate_new_data_batch(10)\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
