{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning!\n",
    "This is a much longer version of \"playing atari\" notebook that utilizes a lot of manual coding.\n",
    "\n",
    "Take a look at \"playing atari with deep reinforcement learning\" and just add a few GRUs and change learning method for the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"tutorial.gym.atari.KungFuMaster-v0.cnn\"\n",
    "snapshot_path = \".\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'KungFuMaster-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 10\n",
    "\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is my machine-specific config. replace if are not me.\n",
    "\n",
    "#theano device selection\n",
    "%env THEANO_FLAGS='device=gpu2'\n",
    "\n",
    "\n",
    "#snapshot path - where neural network snapshots are saved during the main training loop\n",
    "!mkdir ./agentnet_snapshots/\n",
    "snapshot_path = \"./agentnet_snapshots/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use advanced AgentNet techniques \n",
    "\n",
    "__The notebook is built above \"Playing atari with deep reinforcement learning__. Technically, one can produce it from the original notebook by playing with agentnet functionality mentioned in its last cell.\n",
    "\n",
    "All the changes as compared to that notebook will be marked with __[new]__ tag.\n",
    "\n",
    "\n",
    "# [new]\n",
    "\n",
    "* The notebook is mostly based on \"Playing atari with Deep Reinforcement Learning (OpenAI Gym)\" example\n",
    " * All changes against that example will be marked with #[new] sign, like one above\n",
    "* We use a recurrent memory layer, implemented via Gated Recurrent Unit\n",
    "* We use advantage actor-critic method to train agent (using policy + state values instead of q-values)\n",
    "* We train agent to hurt humans in KungFu master game\n",
    "* We also use a bit heavier a network to process inputs (~convolutional from basic example)\n",
    "  * If you have no GPU and want agent to train faster than human child, replace it with what worked in basic example\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Parameters\n",
    "* observation dimensions, actions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print(action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup step by step\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * chooses what action to take given Q-values\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent observations\n",
    "\n",
    "* Here you define where observations (game images) appear in the network\n",
    "* You can use any lasagne architecture you want. We provide several examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "from lasagne.layers import InputLayer,DropoutLayer,DenseLayer, ExpressionLayer, Conv2DLayer,MaxPool2DLayer\n",
    "from lasagne.layers import flatten, dimshuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#image observation\n",
    "observation_layer = lasagne.layers.InputLayer(observation_shape,\n",
    "                                                    name=\"images input\")\n",
    "\n",
    "observation_reshape = lasagne.layers.dimshuffle(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n",
    "\n",
    "observation_small = lasagne.layers.Pool2DLayer(observation_reshape,(2,2),mode='average_exc_pad')\n",
    "\n",
    "\n",
    "#conv\n",
    "cnn = Conv2DLayer(observation_small, \n",
    "                  num_filters=32,\n",
    "                  filter_size=(8,8),\n",
    "                  stride=(4,4),\n",
    "                  name='conv0')\n",
    "cnn = Conv2DLayer(cnn, \n",
    "                  num_filters=64,\n",
    "                  filter_size=(4,4),\n",
    "                  stride=(2,2),\n",
    "                  name='conv2')\n",
    "\n",
    "#dense with dropout    \n",
    "dnn = DenseLayer(cnn,num_units=500,name='dense0')\n",
    "dnn = DropoutLayer(dnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "dnn = DenseLayer(dnn,num_units=500,name='dense1')\n",
    "\n",
    "# [end of that part]\n",
    "\n",
    "inp_nn = dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Agent memory states\n",
    " * Here you can define arbitrary transitions between \"previous state\" variables and their next states\n",
    " * The rules are\n",
    "   * previous states must be input layers\n",
    "   * next states must have same shape as previous ones\n",
    "   * otherwise it can be any lasagne network\n",
    "   * AgentNet.memory has several useful layers\n",
    "   \n",
    " * During training and evaluation, your states will be updated recurrently\n",
    "   * next state at t=1 is given as previous state to t=2\n",
    " \n",
    " * Finally, you have to define a dictionary mapping new state -> previous state\n",
    "\n",
    "\n",
    "Atari game environments are known to have __flickering__ effect where some sprites are shown only on odd frames and others on even ones - that was used to optimize performance at the time.\n",
    "\n",
    "To compensate for this, we shall use the memory layer called __WindowAugmentation__ which basically maintains a K previous time steps of what it is fed with.\n",
    "\n",
    "\n",
    "# [new]\n",
    "We shall also use a GRUMemoryLayer to represent agent's recurrent memory state. This state is updated on every turn given window state.\n",
    "Not that this is __not the same__ as `lasagne.layers.GRULayer` as GRUMemoryLayer only does a single time tick (lasagne version iterates over the whole sequence).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#memory\n",
    "#using simple window-based memory that stores several states\n",
    "#the SpaceInvaders environment does not need any more as it is almost fully-observed\n",
    "from agentnet.memory import WindowAugmentation, GRUMemoryLayer\n",
    "\n",
    "\n",
    "#our window\n",
    "window_size = 3\n",
    "prev_window = InputLayer((None,window_size,inp_nn.output_shape[1]),\n",
    "                                        name = \"previous window state\")\n",
    "\n",
    "window = WindowAugmentation(inp_nn,prev_window,name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "# first (and so far only) GRU layer\n",
    "\n",
    "gru0_size = 256\n",
    "\n",
    "prev_gru0 = InputLayer((None,gru0_size),name=\"previous GRU0 state\")\n",
    "\n",
    "#neuron-wise maxima of window frames as an input to the GRU\n",
    "window_max = ExpressionLayer(window,function=lambda v: v.max(axis=1), \n",
    "                             output_shape=inp_nn.output_shape,\n",
    "                             name = 'window_max')\n",
    "\n",
    "\n",
    "window_max = lasagne.layers.FeaturePoolLayer(window,window_size)\n",
    "\n",
    "gru0 = GRUMemoryLayer(gru0_size,\n",
    "                      observation_input = window_max,\n",
    "                      prev_state_input = prev_gru0,\n",
    "                      name=\"gru0\"\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "#a dictionary that maps next agent memory states to previous ones\n",
    "from collections import OrderedDict\n",
    "memory_dict = OrderedDict([(window,prev_window),\n",
    "                           (gru0,prev_gru0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent policy and action picking\n",
    "\n",
    "\n",
    "# [new]\n",
    "\n",
    "* Since we are using Actor-Critic method, we need to predict 2 values:\n",
    "  * State value - basicly a Q-value of best action in a state\n",
    "  * Agent policy - probabilities of taking actions\n",
    "  \n",
    "\n",
    "* To pick actions, we use a probablistic resolver\n",
    "  * That one picks actions with given probabilities\n",
    "  * We use a laplacian smoothing to pick actions in training (to bolster exploration) \n",
    "  * The resolver output is considered agent's next action and sent into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#State values\n",
    "state_value_layer = DenseLayer(gru0,\n",
    "                               num_units = 1,\n",
    "                               nonlinearity = lasagne.nonlinearities.linear,\n",
    "                               name = \"Vpredicted\")\n",
    "\n",
    "\n",
    "#a2c probabilities\n",
    "\n",
    "\n",
    "policy_layer_pre_softmax = DenseLayer(gru0,\n",
    "                                     num_units = n_actions,\n",
    "                                     nonlinearity= None,\n",
    "                                     name=\"policy_original\")\n",
    "\n",
    "\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "policy_layer = NonlinearityLayer(policy_layer_pre_softmax,\n",
    "                                 lasagne.nonlinearities.softmax)\n",
    "\n",
    "\n",
    "#actual action probablilities\n",
    "epsilon = theano.shared(np.float32(0.1),\"epsilon\")\n",
    "\n",
    "\n",
    "#add laplacian smoothing\n",
    "smooth_policy_layer = NonlinearityLayer(\n",
    "    policy_layer_pre_softmax,\n",
    "    lambda v: lasagne.nonlinearities.softmax(v*(1.-epsilon) + epsilon/n_actions)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#resolver\n",
    "\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "\n",
    "\n",
    "resolver = ProbabilisticResolver(smooth_policy_layer,assume_normalized=True,name=\"resolver\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs\n",
    "\n",
    "# [new]\n",
    "* Note that we can have any (incl. none) number of agent policy variables,\n",
    "* so you can use that to track any layer output\n",
    "* It is also possible to have multiple observations and actions, but that's not aplicable to Atari environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              [policy_layer,state_value_layer],\n",
    "              resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((resolver,state_value_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening in parallel\n",
    "* We define a thin wrapper that stores\n",
    " * game emulators\n",
    " * last agent observations\n",
    " * agent memories at last time tick\n",
    " * a function to generate interaction sessions\n",
    "* This allows us to instantly continue a session from where it stopped\n",
    "\n",
    "\n",
    "\n",
    "The wrapper itself is a class with a single method that creates interaction sessions. It can be found at examples/openai_gym_pool.py\n",
    "\n",
    "\n",
    "Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "\n",
    "__The EnvPool class already contains experience replay and update functionality, but to be more explicit, we re-write them here.__ \n",
    "\n",
    "__If you want shorter version, see the url at the top of the notebook.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,GAME_TITLE, N_PARALLEL_GAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "observation_log,action_log,reward_log,_,_,_  = pool.interact(50)\n",
    "\n",
    "#sanity check: random actions\n",
    "print(np.array(action_names)[np.array(action_log)[:3,:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "\n",
    "Since our network exists in a theano graph and OpenAI gym doesn't, we shall train out network via experience replay.\n",
    "\n",
    "To do that in AgentNet, one can use a SessionPoolEnvironment.\n",
    "\n",
    "It's simple: you record new sessions using `interact(...)`, and than immediately train on them.\n",
    "\n",
    "1. Interact with Atari, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=agent.agent_states.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_pool(env, pool,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "    \n",
    "    #get interaction sessions\n",
    "    observation_tensor,action_tensor,reward_tensor,_,is_alive_tensor,_= pool.interact(n_steps=n_steps)\n",
    "        \n",
    "    #load them into experience replay environment\n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,preceding_memory_states)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,pool,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of training is to store a large pool of sessions and train on random batches of them. \n",
    "* Why that is expected to be better - http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
    "* Or less proprietary - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "To do that, one might make use of\n",
    "* ```env.load_sessions(...)``` - load new sessions\n",
    "* ```env.get_session_updates(...)``` - does the same thing via theano updates (advanced)\n",
    "* ```batch_env = env.sample_session_batch(batch_size, ...)``` - create an experience replay environment that contains batch_size random sessions from env (rerolled each time). Should be used in training instead of env.\n",
    "* ```env.select_session_batch(indices)``` does the same thing deterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (reinforcement learning objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell\n",
    "\n",
    "# [new]\n",
    "* Note that we not unpack several variables (policy, values) instead of Q-values\n",
    "* We than reshape V_seq from (batch, time, 1 unit) intp (batch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replay interaction sequences of length <= replay_seq_len\n",
    "\n",
    "_,observation_seq,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    "    experience_replay=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#observation seq are the observation tensor we just loaded\n",
    "#policy seq are agent action probabilities predicted within experience replay\n",
    "# V_seq are agent state values\n",
    "\n",
    "#The three \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#second - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n",
    "\n",
    "\n",
    "#reshape V_seq from (batch, time, 1 unit) intp (batch_time)\n",
    "V_seq = V_seq[:,:,0]\n",
    "\n",
    "\n",
    "#the actions agent took in the original recorded game\n",
    "action_seq = env.actions[0]\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.rewards\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.is_alive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "\n",
    "# [new]\n",
    "* In this part we are using an n-step Advantage Actor-Critic (A2c)\n",
    "* In this case, it's a 10-step a2c (see n_steps parameter)\n",
    "* To use Q-learning or sarsa, you will have to predict Q-values instead of probabilities\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - actor-critic loss function)\n",
    "  * $ log {\\pi} \\cdot (V_{percieved} - V_{predicted}) $\n",
    "  * $\\pi$ is agent policy, $V$'s are state values\n",
    "  * Read more at http://www.arxiv.org/pdf/1602.01783v1.pdf\n",
    "    \n",
    "* If you want to do it the hard way instead, try .get_reference_state_values and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import a2c_n_step\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.99),name = 'q_learning_gamma')\n",
    "\n",
    "\n",
    "#IMPORTANT!\n",
    "# If you are training on a game that has rewards far outside some [-5,+5]\n",
    "# it is a good idea to downscale them to avoid divergence\n",
    "scaled_reward_seq = rewards_seq\n",
    "#For KungFuMaster, however, not scaling rewards is at least working\n",
    "\n",
    "\n",
    "elwise_a2c_loss = a2c_n_step.get_elementwise_objective(policy_seq,\n",
    "                                                       V_seq,\n",
    "                                                       action_seq,\n",
    "                                                       rewards_seq,\n",
    "                                                       is_alive_seq,\n",
    "                                                       n_steps=10, #using n-step on-policy actor-critic\n",
    "                                                       gamma_or_gammas=gamma,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "a2c_loss = elwise_a2c_loss.sum() / is_alive_seq.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "# [new]\n",
    "We regularize agent's policy with entropy\n",
    " * See here http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.3433&rep=rep1&type=pdf (ref by that article above)\n",
    " * Basically, we punish agent for being too certain on what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-4\n",
    "\n",
    "\n",
    "\n",
    "#regularize with entropy, - H(p(s,theta)). \n",
    "\n",
    "entropy_reg_elwise = - T.sum(policy_seq * T.log(policy_seq),axis=-1)\n",
    "\n",
    "is_bad = T.or_(T.isinf(entropy_reg_elwise), T.isnan(entropy_reg_elwise))\n",
    "\n",
    "entropy_reg_elwise = T.switch(is_bad, 0, entropy_reg_elwise)\n",
    "\n",
    "reg_entropy =  T.sum(entropy_reg_elwise*is_alive_seq)/ is_alive_seq.sum() * 0.05\n",
    "\n",
    "\n",
    "reg = reg_l2 + reg_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = a2c_loss + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,a2c_loss,reg,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session visualization tools\n",
    "\n",
    "__[warning, this thing basicly tries to track various Qvalues over time]__\n",
    "\n",
    "__[but it's bulky and stupid, so don't try to understand it if it didn't come naturally]__\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* in this function, we display game image and than print strings following pattern\n",
    "  * [action_name] ([predicted action proba]) -> reward | next iteration\n",
    "  * each block represents agent's decision in a single time step following the shown game state \n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are probabilities of actions (agent policy\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n",
    "\n",
    "At the beginning, agent knows nothing (Jon) and behaves randomly\n",
    "\n",
    "\n",
    "# [new]\n",
    "We just plug in probabilities instead of Qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        policy_seq, action_seq,rewards_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False,max_n_sessions = 3,update = True):\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in pool.envs[:max_n_sessions]]\n",
    "    \n",
    "    if update:\n",
    "        #load these pics into environment\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "    \n",
    "    \n",
    "    printables = get_printables()\n",
    "    \n",
    "    \n",
    "    for i in range(max_n_sessions):\n",
    "        plt.imshow(pictures[i])\n",
    "        plt.show()\n",
    "            \n",
    "        policy_log,actions_log,reward_log, is_alive_log = map(lambda v: np.array(v[i:i+1]), printables)\n",
    "        \n",
    "\n",
    "        print_sessions(policy_log,actions_log,reward_log,\n",
    "                       is_alive_seq = is_alive_log,\n",
    "                       action_names=action_names,\n",
    "                       legend = True, #do not show legend since there's too many labeled objects\n",
    "                      plot_policy = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "display_sessions(with_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "batch_size= 10\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,pool,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/2500.)\n",
    "        epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%10 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        epsilon.set_value(0)\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        print(\"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy))\n",
    "        print(\"rec %.3f reg %.3f\"%(q_loss,l2_penalty))\n",
    "\n",
    "    if epoch_counter %500 ==0:\n",
    "        print(\"Learning curves:\")\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print(\"Random session examples\")\n",
    "        display_sessions(with_plots=False)\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %1000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print(\"saved\", snap_name)\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials\n",
    " * we took epoch 7k for final submission\n",
    " * done via `load(resolver,\"./agentnet_snapshots/{your experiment_setup_name}.epoch7000.pcl\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random session examples\")\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[m.close() for m in gym.monitoring._open_monitors()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = '/tmp/AgentNet-simplenet-SpaceInvadersv0-Recording0'\n",
    "\n",
    "\n",
    "step = agent.get_react_function()\n",
    "\n",
    "subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "#starting monitor. This setup does not write videos\n",
    "#subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "#this setup does\n",
    "subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "\n",
    "for i_episode in xrange(10):\n",
    "    \n",
    "    #initial observation\n",
    "    observation = subm_env.reset()\n",
    "    #initial memory\n",
    "    prev_memories = [np.zeros((1,) + tuple(mem.output_shape[1:]),\n",
    "                              dtype=getattr(mem,\"output_dtype\",theano.config.floatX))\n",
    "                             for mem in agent.agent_states]\n",
    "    \n",
    "    \n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        agent_output = step([observation],*prev_memories)\n",
    "        action,new_memories = agent_output[0],agent_output[1:]\n",
    "        \n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "        \n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gym.upload(save_path,\n",
    "           \n",
    "           #this notebook\n",
    "           writeup=<url to my gist>, \n",
    "           \n",
    "           #your api key\n",
    "           api_key=<my_own_api_key>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
