{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"tutorial.gym.atari.pendulum-v0.cnn\"\n",
    "snapshot_path = \".\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'Pendulum-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 3\n",
    "\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=cpu'\n",
      "mkdir: cannot create directory `./agentnet_snapshots/': File exists\r\n"
     ]
    }
   ],
   "source": [
    "#this is my machine-specific config. replace if are not me.\n",
    "\n",
    "#theano device selection\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "\n",
    "\n",
    "#snapshot path - where neural network snapshots are saved during the main training loop\n",
    "!mkdir ./agentnet_snapshots/\n",
    "snapshot_path = \"./agentnet_snapshots/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use advanced AgentNet techniques \n",
    "\n",
    "\n",
    "# [new]\n",
    "\n",
    "* The notebook is mostly based on \"Playing atari with Deep Reinforcement Learning (OpenAI Gym)\" example\n",
    " * All changes against that example will be marked with #[new] sign, like one above\n",
    "* We use a recurrent memory layer, implemented via Gated Recurrent Unit\n",
    "* We use advantage actor-critic method to train agent (using policy + state values instead of q-values)\n",
    "* We train agent to hurt humans in KungFu master game\n",
    "* We also use a bit heavier a network to process inputs (~convolutional from basic example)\n",
    "  * If you have no GPU and want agent to train faster than human child, replace it with what worked in basic example\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-05-19 21:12:18,318] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.99831404,  0.05804377, -0.79935278])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "#plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Parameters\n",
    "* observation dimensions, actions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_actions = atari.action_space.shape[0]\n",
    "observation_shape = (None,)+atari.observation_space.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atari.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup step by step\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * chooses what action to take given Q-values\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent observations\n",
    "\n",
    "* Here you define where observations (game images) appear in the network\n",
    "* You can use any lasagne architecture you want. We provide several examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "from lasagne.layers import InputLayer,DropoutLayer,DenseLayer, ExpressionLayer, Conv2DLayer,MaxPool2DLayer\n",
    "from lasagne.layers import flatten, dimshuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#image observation\n",
    "observation_layer = lasagne.layers.InputLayer(observation_shape,\n",
    "                                                    name=\"images input\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dense with dropout    \n",
    "dnn = DenseLayer(observation_layer,num_units=150,name='dense0')\n",
    "dnn = DropoutLayer(dnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "dnn = DenseLayer(dnn,num_units=30,name='dense1')\n",
    "\n",
    "# [end of that part]\n",
    "\n",
    "inp_nn = dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Agent memory states\n",
    " * Here you can define arbitrary transitions between \"previous state\" variables and their next states\n",
    " * The rules are\n",
    "   * previous states must be input layers\n",
    "   * next states must have same shape as previous ones\n",
    "   * otherwise it can be any lasagne network\n",
    "   * AgentNet.memory has several useful layers\n",
    "   \n",
    " * During training and evaluation, your states will be updated recurrently\n",
    "   * next state at t=1 is given as previous state to t=2\n",
    " \n",
    " * Finally, you have to define a dictionary mapping new state -> previous state\n",
    "\n",
    "\n",
    "Atari game environments are known to have __flickering__ effect where some sprites are shown only on odd frames and others on even ones - that was used to optimize performance at the time.\n",
    "\n",
    "To compensate for this, we shall use the memory layer called __WindowAugmentation__ which basically maintains a K previous time steps of what it is fed with.\n",
    "\n",
    "\n",
    "# [new]\n",
    "We shall also use a GRUMemoryLayer to represent agent's recurrent memory state. This state is updated on every turn given window state.\n",
    "Not that this is __not the same__ as `lasagne.layers.GRULayer` as GRUMemoryLayer only does a single time tick (lasagne version iterates over the whole sequence).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a dictionary that maps next agent memory states to previous ones\n",
    "from collections import OrderedDict\n",
    "memory_dict = OrderedDict([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent policy and action picking\n",
    "\n",
    "\n",
    "# [new]\n",
    "\n",
    "* Since we are using Actor-Critic method, we need to predict 2 values:\n",
    "  * State value - basicly a Q-value of best action in a state\n",
    "  * Agent policy - probabilities of taking actions\n",
    "  \n",
    "\n",
    "* To pick actions, we use a probablistic resolver\n",
    "  * That one picks actions with given probabilities\n",
    "  * We use a laplacian smoothing to pick actions in training (to bolster exploration) \n",
    "  * The resolver output is considered agent's next action and sent into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scaled_tanh(x):\n",
    "    tanh = lasagne.nonlinearities.tanh(x)/2+0.5\n",
    "    tanh *=  np.float32(atari.action_space.high - atari.action_space.low)\n",
    "    tanh += np.float32(atari.action_space.low)\n",
    "    return tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#policy\n",
    "policy_layer= DenseLayer(inp_nn,\n",
    "                         num_units = n_actions,\n",
    "                         nonlinearity = scaled_tanh,\n",
    "                         name = \"mu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from lasagne.layers import concat\n",
    "#State values\n",
    "state_value_layer = DenseLayer(concat([inp_nn,policy_layer]),\n",
    "                               num_units = 1,\n",
    "                               nonlinearity = lasagne.nonlinearities.linear,\n",
    "                               name = \"Vpredicted\")\n",
    "\n",
    "\n",
    "#resolver\n",
    "\n",
    "\n",
    "resolver = policy_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs\n",
    "\n",
    "# [new]\n",
    "* Note that we can have any (incl. none) number of agent policy variables,\n",
    "* so you can use that to track any layer output\n",
    "* It is also possible to have multiple observations and actions, but that's not aplicable to Atari environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              [policy_layer,state_value_layer],\n",
    "              resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dense0.W,\n",
       " dense0.b,\n",
       " dense1.W,\n",
       " dense1.b,\n",
       " mu.W,\n",
       " mu.b,\n",
       " Vpredicted.W,\n",
       " Vpredicted.b]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((resolver,state_value_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor: [dense0.W, dense0.b, dense1.W, dense1.b, mu.W, mu.b]\n",
      "critic: [dense0.W, dense0.b, dense1.W, dense1.b, Vpredicted.W, Vpredicted.b]\n"
     ]
    }
   ],
   "source": [
    "actor_weights = filter(lambda w: not w.name.startswith(\"Vpredicted\"),weights)\n",
    "critic_weights = filter(lambda w: not w.name.startswith(\"mu\"),weights)\n",
    "print 'actor:',actor_weights\n",
    "print 'critic:',critic_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* Compute action and next state given observation and prev state\n",
    "* The code was written in a generic way and did not undergo any changes since previous turorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "applier_observation = T.matrix(\"input image\",dtype=floatX)\n",
    "\n",
    "\n",
    "# inputs to all agent memory states (usng lasagne defaults, may use any theano inputs)\n",
    "applier_memories = OrderedDict([ (new_st,prev_st.input_var)\n",
    "                                for new_st, prev_st in agent.state_variables.items()\n",
    "                               ])\n",
    "\n",
    "\n",
    "res =agent.get_agent_reaction(applier_memories,\n",
    "                              applier_observation,\n",
    "                              deterministic = True #disable dropout here. Only enable in experience replay\n",
    "                             )\n",
    "\n",
    "\n",
    "applier_actions,applier_new_states,applier_policy = res\n",
    "\n",
    "applier_fun = theano.function([applier_observation]+applier_memories.values(),\n",
    "        applier_actions+applier_new_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a nice pythonic interface\n",
    "def step(observation, prev_memories = 'zeros',batch_size = N_PARALLEL_GAMES):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((batch_size,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype=floatX) \n",
    "                         for mem in agent.state_variables]\n",
    "    \n",
    "    res = applier_fun(np.array(observation,dtype=floatX),*prev_memories)\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action,memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* We define a small container that stores\n",
    " * game emulators\n",
    " * last agent observations\n",
    " * agent memories at last time tick\n",
    "* This allows us to instantly continue a session from where it stopped\n",
    "\n",
    "\n",
    "\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-05-19 21:12:48,293] Making new env: Pendulum-v0\n",
      "[2016-05-19 21:12:48,294] Making new env: Pendulum-v0\n",
      "[2016-05-19 21:12:48,295] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "#A whole lot of space invaders\n",
    "\n",
    "class GamePool:\n",
    "    def __init__(self,game_title,n_games):\n",
    "        \"\"\"\n",
    "        A pool that stores several\n",
    "           - game states (gym environment)\n",
    "           - prev_observations - last agent observations\n",
    "           - prev memory states - last agent hidden states\n",
    "           \n",
    "       \"\"\"\n",
    "        \n",
    "        self.ataries = [gym.make(game_title) for i in range(n_games)]\n",
    "\n",
    "        self.prev_observations = [atari.reset() for atari in self.ataries]\n",
    "    \n",
    "        self.prev_memory_states = 'zeros'\n",
    "\n",
    "pool = GamePool(GAME_TITLE, N_PARALLEL_GAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a function that creates and records environment interaction sessions\n",
    "def interact(pool,n_steps = 100,verbose=False):\n",
    "    \"\"\"generate interaction sessions with ataries (openAI gym atari environments)\n",
    "    Sessions will have length n_steps. \n",
    "    Each time one of games is finished, it is immediately getting reset\"\"\"\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = pool.prev_observations \n",
    "    \n",
    "    prev_memory_states = pool.prev_memory_states\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        actions,new_memory_states = step(prev_observations,prev_memory_states)\n",
    "\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = \\\n",
    "            zip(*map(\n",
    "                     lambda atari, action: atari.step(action), \n",
    "                     pool.ataries,actions))\n",
    "            \n",
    "        new_observations = np.array(new_observations,dtype=floatX)\n",
    "        \n",
    "        for i in range(len(pool.ataries)):\n",
    "            if is_done[i]:\n",
    "                new_observations[i] = pool.ataries[i].reset()\n",
    "                \n",
    "                for m_i in range(len(new_memory_states)):\n",
    "                    new_memory_states[m_i][i] = 0\n",
    "                    \n",
    "                if verbose:\n",
    "                    print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,new_memory_states,is_done,infos))\n",
    "        \n",
    "        prev_observations = new_observations\n",
    "        prev_memory_states = new_memory_states\n",
    "                \n",
    "    pool.prev_memory_states = prev_memory_states\n",
    "    pool.prev_observations = prev_observations\n",
    "    \n",
    "    return zip(*history_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.56900235  -9.82291853  -9.78350679  -9.51834233  -9.25045901\n",
      "   -8.99015961  -8.74772896  -8.53266142  -8.35308424  -8.21513536]\n",
      " [ -1.51351801  -1.81074812  -2.28367345  -2.93929474  -3.79060608\n",
      "   -4.84812236  -6.1082559   -7.54571456  -9.10440429 -10.70400399]\n",
      " [ -0.23210503  -0.26389161  -0.34842399  -0.50113246  -0.74715915\n",
      "   -1.12551119  -1.69297292  -2.52592965  -3.71738945  -5.36672985]]\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 17.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "observation_log,action_log,reward_log,_,_,_  = interact(pool,50)\n",
    "\n",
    "\n",
    "print np.array(reward_log)[:10].T\n",
    "#print np.array(action_names)[np.array(action_log)[:3,:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "\n",
    "Since our network exists in a theano graph and OpenAI gym doesn't, we shall train out network via experience replay.\n",
    "\n",
    "To do that in AgentNet, one can use a SessionPoolEnvironment.\n",
    "\n",
    "It's simple: you record new sessions using `interact(...)`, and than immediately train on them.\n",
    "\n",
    "1. Interact with Atari, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_pool(env, pool,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    \n",
    "    observation_log,action_log,reward_log,_,is_done_log,_= interact(pool,n_steps=n_steps)\n",
    "    \n",
    "    \n",
    "    #tensor dimensions\n",
    "    \n",
    "    # [batch_i, time_i, width, height, rgb]\n",
    "    observation_tensor = np.array(observation_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i,time_i]\n",
    "    action_tensor = np.array(action_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i, time_i]\n",
    "    reward_tensor = np.array(reward_log).swapaxes(0,1)\n",
    "\n",
    "    # [batch_i, time_i]\n",
    "    is_alive_tensor = 1- np.array(is_done_log,dtype = 'int8').swapaxes(0,1)\n",
    "    \n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,[])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,pool,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of training is to store a large pool of sessions and train on random batches of them. \n",
    "* Why that is expected to be better - http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
    "* Or less proprietary - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "To do that, one might make use of\n",
    "* ```env.load_sessions(...)``` - load new sessions\n",
    "* ```env.get_session_updates(...)``` - does the same thing via theano updates (advanced)\n",
    "* ```batch_env = env.sample_session_batch(batch_size, ...)``` - create an experience replay environment that contains batch_size random sessions from env (rerolled each time). Should be used in training instead of env.\n",
    "* ```env.select_session_batch(indices)``` does the same thing deterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (reinforcement learning objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell\n",
    "\n",
    "# [new]\n",
    "* Note that we not unpack several variables (policy, values) instead of Q-values\n",
    "* We than reshape V_seq from (batch, time, 1 unit) intp (batch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/yozhik/AgentNet/agentnet/agent/mdp_agent.py:315: UserWarning: You are using experience replay environment as normal environment. This will work, but you can geta free performance boost by using passing optimize_experience_replay = True to .get_sessions\n",
      "  warn(\"You are using experience replay environment as normal environment. This will work, but you can get\"\\\n"
     ]
    }
   ],
   "source": [
    "#replay interaction sequences of length <= replay_seq_len\n",
    "\n",
    "_,observation_seq,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#observation seq are the observation tensor we just loaded\n",
    "#policy seq are agent action probabilities predicted within experience replay\n",
    "# V_seq are agent state values\n",
    "\n",
    "#The three \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#second - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n",
    "\n",
    "\n",
    "#reshape V_seq from (batch, time, 1 unit) intp (batch_time)\n",
    "V_seq = V_seq[:,:,0]\n",
    "\n",
    "\n",
    "#the actions agent took in the original recorded game\n",
    "action_seq = env.actions[0]\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.rewards\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.is_alive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "\n",
    "# [new]\n",
    "* In this part we are using an n-step Advantage Actor-Critic (A2c)\n",
    "* In this case, it's a 10-step a2c (see n_steps parameter)\n",
    "* To use Q-learning or sarsa, you will have to predict Q-values instead of probabilities\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - actor-critic loss function)\n",
    "  * $ log {\\pi} \\cdot (V_{percieved} - V_{predicted}) $\n",
    "  * $\\pi$ is agent policy, $V$'s are state values\n",
    "  * Read more at http://www.arxiv.org/pdf/1602.01783v1.pdf\n",
    "    \n",
    "* If you want to do it the hard way instead, try .get_reference_state_values and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import dpg_n_step\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.99),name = 'q_learning_gamma')\n",
    "\n",
    "\n",
    "#IMPORTANT!\n",
    "# If you are training on a game that has rewards far outside some [-5,+5]\n",
    "# it is a good idea to downscale them to avoid divergence\n",
    "scaled_reward_seq = rewards_seq\n",
    "#For KungFuMaster, however, not scaling rewards is at least working\n",
    "\n",
    "\n",
    "elwise_actor_loss,elwise_critic_loss = dpg_n_step.get_elementwise_objective_components(policy_seq,\n",
    "                                                       rewards_seq,\n",
    "                                                       V_seq,\n",
    "                                                       V_seq,\n",
    "                                                       is_alive_seq,\n",
    "                                                       n_steps=10, #using n-step on-policy actor-critic\n",
    "                                                       gamma_or_gammas=gamma,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "actor_loss = elwise_actor_loss.sum() / is_alive_seq.sum()\n",
    "critic_loss = elwise_critic_loss.sum() / is_alive_seq.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "# [new]\n",
    "We regularize agent's policy with entropy\n",
    " * See here http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.3433&rep=rep1&type=pdf (ref by that article above)\n",
    " * Basically, we punish agent for being too certain on what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_grads = dict(zip(actor_weights,T.grad(actor_loss,actor_weights)))\n",
    "critic_grads = dict(zip(critic_weights,T.grad(critic_loss,critic_weights)))\n",
    "\n",
    "\n",
    "grads = dict(actor_grads)\n",
    "for param,cgrad in critic_grads.items():\n",
    "    if param in grads:\n",
    "        grads[param] += cgrad\n",
    "    else:\n",
    "        grads[param] = cgrad\n",
    "        \n",
    "        \n",
    "grads = [grads[w] for w in weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = lasagne.updates.adadelta(grads,\n",
    "                                             weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[actor_loss+critic_loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[actor_loss+critic_loss,actor_loss,critic_loss,mean_session_reward])#!!!!fix losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tools for model persistence (in progress now. Requires unique names)\n",
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = -40.\n",
    "ma_reward_greedy =-40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100000\n",
    "batch_size= 10\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,pool,replay_seq_len)\n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%5 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "        if epoch_counter %500 ==0:\n",
    "            print \"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "                epoch_counter,full_loss,float('inf'),ma_reward_current,ma_reward_greedy)\n",
    "            print \"rec %.3f reg %.3f\"%(q_loss,l2_penalty)\n",
    "\n",
    "    if epoch_counter %500 ==0:\n",
    "        print \"Learning curves:\"\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %10000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print \"saved\", snap_name\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials\n",
    " * we took epoch 7k for final submission\n",
    " * done via `load(resolver,\"./agentnet_snapshots/{your experiment_setup_name}.epoch7000.pcl\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random session examples\"\n",
    "!!! !!display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
