{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_setup_name = \"wikicat.long_run.category_focused.10s_batch.nesterov.3layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "mkdir: cannot create directory '/root/agentnet_snapshots': File exists\r\n"
     ]
    }
   ],
   "source": [
    "#import everything in the world\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano\n",
    "theano.config.floatX = 'float32'\n",
    "theano.config.openmp = True\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "!mkdir ~/agentnet_snapshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "floatX = theano.config.floatX\n",
    "\n",
    "from auxilary import _shared,set_shared,_in1d\n",
    "from persistence import save,load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эта тетрадка рассказывает нелёгком пути обучения\n",
    "* Recurrent Q-network \n",
    "* two layers of GRU memory\n",
    "* Qvalues predicted via dense layer from both memory layers\n",
    "* epsilon-greedy decision making\n",
    "* trained with Nesterov Mommentum SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи\n",
    "* В начале сетке даётся некоторый человек, про которого она пока ничего не знает\n",
    "* Человек - музыкант или околомузыкальная личность (с меньшей вероятностью - группа или... страна)\n",
    "* Про человека известен некоторый набор фактов\n",
    " * факты типа \"Известно, что\" - за то, что какой-то аттрибут есть в его вики-карточке\n",
    "   * например Nickname:is_known - есть прозвище, Died:is_known - есть дата смерти\n",
    " * Категориальные факты\n",
    "   * инструменты, на которых играет человек\n",
    "   * происхождение\n",
    "   * декады активности\n",
    "   * и т.п.\n",
    "\n",
    "\n",
    "* За один шаг сетка может\n",
    " * сказать, что какой-то аттрибут про пользователя есть\n",
    "   * например, Nickname:is_known - предположение, что у пользователя есть прозвище\n",
    "   * если догадка верна, сеть получает положительное подкрепление (\"конфетку\")\n",
    "   * если нет - отрицательное (\"подзатыльник\")\n",
    " * сказать \"больше ничего не угадаю, несите следующего\"\n",
    "   * нулевое подкрепление\n",
    "   * случайно выбирается следующий пользователь\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# данные\n",
    "* Всего в выборке 393 аттрибута\n",
    " * примеры аттрибутов в табе ниже\n",
    "* В данном случае сеть обучается на чуть менее, чем 2500 случайных людей\n",
    "* всего в выборке почти 100к людей/групп/всякого трэша"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#вгрузим данные про кучу музыкантов \n",
    "#как оно собирается - ./data/preprocessing.ipynb\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_csv(\"../data/musicians_categorized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0346481118134\n",
      "(22612, 393)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFAZJREFUeJzt3W2MXFd5wPH/1msXQjwkKyTHdlLZBKeJq7SFlAS1oQyU\nWAbR2P0SO1JTN6RIyC2kagXE6QcvX6hLxUuqKkhtCHUouHKBWk4Lrk3wqJEAu0DeyMa1HWGaDXih\nisGLCsKutx/O2exkfO/szp3ZuXfm/H/S6N459+3s8fiZM889916QJEmSJEmSJEmSJEmSJElShT0I\nTAFPZSz7c+ACMNZUtgM4ARwDNjSV3xD3cQK4b1FqKknqiTcCr+XiwH8VcAD4DnOBfz3wOLAUWAOc\nBEbisqPAjXH+i8DGRauxJKmtX5hn+aPAmYzyjwLvbynbBOwBzgGnCIH/JmAlsJwQ/AEeAjYXq64k\nqVvzBf4sm4BJ4MmW8lWxfNYksDqj/PlYLkkqwWiH618C3Avc0lQ2krOuJKmCOg38VxPy90/E91cC\n3ySkdJ4n5P5pWjYZy69sKX8+c+dXXz3z7LPPdlglSUres8BrernDNWSP6oHsk7vLgLWxIrO/Bo4Q\nvhxGaH9yd6Zqdu7cWXYVLmKdFq6K9bJOC2OdFg6Y6SSoz5fj3wN8FbgGeA64szVQN81PAHvj9EvA\n9qbl24EHCMM5TxJGBEmSSjBfquf2eZa/uuX9h+Kr1TeB6xdaKUnS4ikyqicp9Xq97CpcxDotXBXr\nZZ0WxjotnqqNyInpKknSQo2MjEAH8dwevyQlxsAvarUxRkZGLnrVamPzbyxp4JjqUfyZmNXuI/jv\nIVWfqR5JUlsGfklKjIFfheSdF/DcgFR95vhVKMefv0377ST1Xqc5/k5v0qakjM5+oCQNEQO/2jhP\nu169pMFkjl+LYNTcv1RhVeu2meMvQbscf/sef6fLzP1Li8Fx/MrUbhSOpLRU7X+9Pf5FMt8oHHv8\n0uCyxy9JasvAL0mJMfBLUmIM/JKUGAO/JCXGwC9JiTHwS1JiDPySlJj5Av+DwBTwVFPZXwPPAE8A\nXwBe2bRsB3ACOAZsaCq/Ie7jBHBfd1WWJHVjvsD/KWBjS9lB4FeAXwOOE4I9wHpgS5xuBO5n7kqy\nTwB3Aeviq3WfkqQ+mS/wPwqcaSk7BFyI80eAK+P8JmAPcA44BZwEbgJWAsuBo3G9h4DN3VRaklRc\ntzn+dwJfjPOrgMmmZZPA6ozy52O5eswbsUlaiG4exPIXwM+Bz/aoLgCMj4+/OF+v16nX673c/VCb\nnj6DD06Rhl+j0aDRaBTefiHRYA3wMHB9U9kfAu8Cfgf4WSy7J053xekBYCfwXeAwcF0svx14E/Du\njGN5d84uFLsDZ7tl3p1TGgT9uDvnRuB9hJz+z5rK9wNbgWXAWsJJ3KPAaeAsId8/AtwB7CtwXElS\nD8yX6tlD6J2/CniO0IPfQQjuh+I6XwO2AxPA3jg9H8tmu3fbgX8AXk44J3CgV3+AJKkzVUv8murp\ngqkeKU0+iEWS1JaBX5ISY+CXpMQY+CUpMQZ+SUqMgV+SEmPgl6TEGPjVR6O5N5Gr1cbKrpyUDC/g\nGjC12li8GVueal/A1W4b/+2lYjq9gMvAP2B6f3Vuu2UGfmkQeOWuJKktA78kJcbAL0mJMfBLUmIM\n/JKUGAO/JCXGwK/Kq9XGvOhL6iHH8Q+YFMfx5//Njv2XwHH8kqR5GPglKTEGfklKjIFfkhJj4Jek\nxMwX+B8EpoCnmsrGgEPAceAgcFnTsh3ACeAYsKGp/Ia4jxPAfd1VWZLUjfkC/6eAjS1l9xAC/zXA\nI/E9wHpgS5xuBO5nbnjRJ4C7gHXx1bpPSVKfzBf4HwVan/pxK7A7zu8GNsf5TcAe4BxwCjgJ3ASs\nBJYDR+N6DzVtI0nqsyI5/hWE9A9xuiLOrwImm9abBFZnlD8fyyVJJRjtcvsZ8i/FLGR8fPzF+Xq9\nTr1e7+XuJWngNRoNGo1G4e0XconvGuBh4Pr4/hhQB04T0jiHgWuZy/XvitMDwE7gu3Gd62L57cCb\ngHdnHMtbNkTtn63rLRvm20ZKST9u2bAf2BbntwH7msq3AsuAtYSTuEcJXxBnCfn+EeCOpm2UIwT9\nmYyXJHVnvsC/B/gq8MvAc8CdhB79LYThnG9hroc/AeyN0y8B25mLVNuBBwjDOU8Sfg0kIe/Okt5d\nUlJZvDvnIpvvbppF0humemYtBc5nbrN8+eWcPftCzrGk4dJpqqfbk7tSic6T90UyPV21Po1UHd6y\nQZISY+CXpMQY+CUpMQZ+SUqMgV+SEmPgl6TEGPglKTEGfklKjIFfkhJj4JekxBj4JSkxBn5JSoyB\nX5ISY+CXpMQY+CUpMQZ+SUqMgV+SEmPgl6TEGPh7JO+h6pJUNT5zt0emp8+Q/4BxSaoOe/ySlBgD\nvyQlppvAvwN4GngK+Czwi8AYcAg4DhwELmtZ/wRwDNjQxXElSV0oGvjXAO8CXgdcDywBtgL3EAL/\nNcAj8T3AemBLnG4E7u/i2JKkLhQNvmeBc8AlhBPElwDfA24Fdsd1dgOb4/wmYE/c5hRwErix4LEl\nSV0oGvhfAD4C/Dch4P+I0NNfAUzFdabie4BVwGTT9pPA6oLHliR1oehwzquBPyWkfH4M/DPw+y3r\nzJA9vrF5+UXGx8dfnK/X69Tr9YJVlKTh1Gg0aDQahbcvOsh8C3AL8Efx/R3AG4C3AG8GTgMrgcPA\ntczl+nfF6QFgJ3CkZb8zMzPtviuqK1yslTeOP+9vGiHv7y26v94u6+/+et0Wg/pZkjoVLxZdcDwv\nmuo5Rgj0L48HeyswATwMbIvrbAP2xfn9hJO/y4C1wDrgaMFjlybv6lyv0JU0SIqmep4AHgK+AVwA\nvgX8HbAc2AvcRTiJe1tcfyKWTwDnge20TwNVUv7VueAVupIGRdWiVaVTPfkpB6hKesNUz/z7k4ZN\nv1I9kqQBZeDXkBrNPR9Tq42VXTmpVN6dU0PqPHlpoOnpqmU4pf6yxy9JiTHwS1JiDPwtHKsvadiZ\n42/hWH1Jw84evyQlxsAvSYlJNvDn5fIladglm+PPz+Ub/CUNt2R7/EqZV/Uqbcn2+JUyr+pV2uzx\nS1JiDPySlBgDvyQlxsAvSYkx8EtSYgz8kpQYA78kJcbAL0mJMfBLUmIM/JKUmG4C/2XA54BngAng\nJmAMOAQcBw7GdWbtAE4Ax4ANXRxXktSFbgL/fcAXgeuAXyUE9HsIgf8a4JH4HmA9sCVONwL3d3ls\nSVJBRYPvK4E3Ag/G9+eBHwO3Artj2W5gc5zfBOwBzgGngJPAjQWPLUnqQtHAvxb4IfAp4FvA3wOv\nAFYAU3GdqfgeYBUw2bT9JLC64LElSV0oelvmUeB1wJ8A/wl8nLm0zqwZ8p9aTt6y8fHxF+fr9Tr1\ner1gFSVpODUaDRqNRuHti958/Arga4SeP8DNhJO3rwbeDJwGVgKHgWuZ+1LYFacHgJ3AkZb9zszM\ntPuu6J3wmMW8J3Dl1aHIsvbb5P29/atfu2VptkW/PoNSr8THxi44nhdN9ZwGniOcxAV4K/A08DCw\nLZZtA/bF+f3AVmAZ4ctiHXC04LEXLO+5uj5bV1LKunkC13uAzxCC+bPAncASYC9wF+Ek7m1x3YlY\nPkE4Ebyd9mmgnsh/ri74bF1Jqapa9Otpqic/RQCmNxayLM22MNWjQdOvVI8kaUAZ+KWXGM08J1Sr\njZVdMalnusnxS0PoPFlpoOnpqmVFpeLs8UtSYgz8kpQYA78kJcbAL0mJMfBLUmIM/JKUGAO/JCXG\nwC9JiTHwS1JiDPySlBgDvyQlxsAvSYkx8EtSYgz8kpQYA78kJcbAL0mJMfBLUmIGPvDXamOZj8qL\nDx+WJLUY+MA/PX2G8Ki8rJfUK9nP4vV5vBpEPnNXWpDsZ/GCz+PV4Om2x78EeAx4OL4fAw4Bx4GD\nwGVN6+4ATgDHgA1dHleSVFC3gf9uYIK5rtA9hMB/DfBIfA+wHtgSpxuB+3twbElSAd0E3yuBtwMP\nALO/dW8Fdsf53cDmOL8J2AOcA04BJ4Ebuzi2JKmgbgL/x4D3AReaylYAU3F+Kr4HWAVMNq03Cazu\n4tiSpIKKntx9B/ADQn6/nrPOfENrMpeNj4+/OF+v16nX83YvSWlqNBo0Go3C2xcdjvAh4A7CUIeX\nATXgC8DrCV8Ep4GVwGHgWuZy/bvi9ACwEzjSst+ZmZnOhmGG8fp52xRZ1t/95f29+X9Xr+vXbplt\nsdBtOv3cSr0Ur1tacDwvmuq5F7gKWAtsBb5C+CLYD2yL62wD9sX5/XG9ZXGbdcDRgseWJHWhV+P4\nZ7s7u4C9wF2Ek7i3xfKJWD5B+JWwHa+w0tAYzb1SfPnyyzl79oU+10dqr2pXnpjqmV0yAOkN22Jh\ny0wDabH1K9UjSRpQBn5JSoyBX5ISY+CXpMQY+CUpMQZ+SUqMgV+SEmPgl6TEGPglKTEGfklKjIFf\nkhJj4JekxBj4JSkxBn5JSoyBX1pU4V79ra9abazsiilhvXoQi6RM58m6V//0dNUehaGU2OOXpMQY\n+CUpMQZ+SUrMwAT+Wm0s8ySZJKkzA3Nyd3r6DPkPwZYkLdTA9Pil4ZI9zNOhnuqHgenxS8Mle5gn\nONRTi69oj/8q4DDwNPBt4L2xfAw4BBwHDgKXNW2zAzgBHAM2FDyuJKlLRbsWV8TX48ClwDeBzcCd\nwP8AHwY+AFwO3AOsBz4LvB5YDXwZuAa40LLfmZmZ7F5QOJGbl+PP3qbYsv7ur/y/t90y26Ks/eW1\nhZQlDnRZcDwv2uM/TQj6AD8BniEE9FuB3bF8N+HLAGATsAc4B5wCTgI3Fjy2JKkLvTi5uwZ4LXAE\nWAFMxfKp+B5gFTDZtM0k4YtCktRn3Z7cvRT4PHA3MN2ybIb837LkLRsfH39xvl6vU6/Xu6qgJA2b\nRqNBo9EovH03wweWAv8KfAn4eCw7BtQJqaCVhBPA1xLy/AC74vQAsJPwK6GZOf7ZJea155bYFlJb\n/crxjwCfBCaYC/oA+4FtcX4bsK+pfCuwDFgLrAOOFjy2lKy8K9gd+69OFO3x3wz8B/Akc92WHYRg\nvhf4JcJJ3NuAH8Xl9wLvJAxgvhv494z92uOfXWIvd26JbTG3pE1b+CshXZ32+Kt2pYiBf3aJwW5u\niW0xt8TArwz9SvVIkgaUgV+SEmPgl6TEGPglKTEGfmkoeJtnLZy3ZZaGgrd51sLZ45ekxBj4JSkx\nBn5JSoyBX5ISY+CXpMQY+CUpMQZ+aejlj/EfGVnm2P8EOY5fGnr5Y/zz7hLq2P/hZo9fkhJj4JeU\nwVtADDNTPZIyeAuIYWaPX5ISU7ke/09/+tOLypYsWVJCTSRpOFUu8GflD0dHDfyS1CuVC/znz1/c\n479wYWkJNZGUbXT24d4XWb78cs6efaHP9VGnzPFL6tDsid+LX9PT044GGgD9DvwbgWPACeADfT62\npEXX7kvhTJkVU5N+Bv4lwN8Sgv964Hbguj4ev6BG2RXI0Ci7AhkaZVcgR6PsCmRolF2BDI0+HCP7\n2oC8XwKNRj/q1Jkq1qmIfgb+G4GTwCngHPBPwKY+Hr+gRtkVyNAouwIZGmVXIEej7ApkaJRdgQyN\nPhwj+9dA3i+B+YJsrTbW95SSgb9zq4Hnmt5PxjJJ6lj4wlj88wzNXzAf/OAHh+K8RT9H9eTdJeol\narXfvahsevr/el4ZSVWRP0roox/9mwKjhNpddbw091iwlJCMyDK7v/H4mt1f/lXMtdpY7q+Zskc/\n9fPa6zcQWmxjfL8DuAD8VdM6J4Gr+1gnSRoGzwKvKbsSWUYJlVsDLAMeZyBO7kqSuvE24L8IPfsd\nJddFkiRJUr9U9cKuU8CTwGPA0ZLq8CAwBTzVVDYGHAKOAweByypQp3HCSK3H4mvjxZstqquAw8DT\nwLeB98byMtsqr07jlNdWLwOOEFKtE8BfxvKyP1N59Rqn3M8VhGuQHgMeju/LbqusOo1Tfjt1ZAkh\n9bOGcFq9Srn/7xD+kcv0RuC1vDTIfhh4f5z/ALCrAnXaCfxZn+vR7Arg1+P8pYSU4nWU21Z5dSq7\nrS6J01Hg68DNlP+ZyqtX2W1FPP5ngP3xfRXaqrVOHbVTFe7VU/ULu8p+6sSjQOuYsFuB3XF+N7C5\nrzXKrhOU21anCZ0GgJ8AzxCuEymzrfLqBOW21f/G6TJCx+sM5X+m8uoF5bbVlcDbgQea6lF2W2XV\naYQO2qkKgb/KF3bNAF8GvgG8q+S6NFtBSLUQpytKrEuz9wBPAJ+knJ+/s9YQfpEcoTptNVunr8f3\nZbbVLxC+kKaYS0VVoZ2y6gXlttXHgPcRhp7PKrutsuo0QwftVIXAv6ALu0ryW4T/rG8D/piQ4qia\n2UsWy/YJYC0htfF94CMl1eNS4PPA3cB0y7Ky2upS4HOEOv2E8tvqQjz2lcBvA29uWV5WO7XWq065\nbfUO4AeEnHleb7rfbZVXp47aqQqB/3nCSbBZVxF6/VXw/Tj9IfAvhLRUFUwR8scAKwkfhLL9gLn/\nBA9QTlstJQT9TwP7YlnZbTVbp39sqlMV2grgx8C/ATdQfjs1m63Xb1BuW/0mIa3zHWAP8BbCZ6vM\ntsqq00N02E5VCPzfANYxd2HXFuZOWJTpEmB5nH8FsIGXnsws035gW5zfxlxAKdPKpvnfo/9tNUL4\niTsBfLypvMy2yqtTmW31KubSAC8HbiH0Hsv+TOXV64qmdfrdVvcSOqJrga3AV4A7KLetsur0B5T/\n/6+QKl7YtZaQb3ycMBSvrHrtAb4H/JxwLuROwkijL1PecLLWOr2T0Ot4kpBj3Ef/8543E1IFj/PS\nIW1ltlVWnd5GuW11PfCtWKcnCbliKP8zlVevsj9Xs97EXIe07LaaVW+q06epRjtJkiRJkiRJkiRJ\nkiRJkiRJkiRJUtr+H67t/kKyoBH0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bd92a5650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#гистограмма количества аттрибутов известных про музыканта\n",
    "plt.hist((df.values).sum(axis=1),bins=50)\n",
    "print (df.values).mean()\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего самплов (8594, 393)\n"
     ]
    }
   ],
   "source": [
    "#отрежем тех, про кого известно менее 5 аттрибутов\n",
    "df =  df[df.values.sum(axis=1) > 15]\n",
    "print \"всего самплов\", df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split categories and attributes\n",
    "feature_names = df.columns\n",
    "categorical_columns = np.nonzero(map(lambda s: s.startswith(\"category:\"),feature_names))[0]\n",
    "attribute_columns = np.nonzero(map(lambda s: not s.startswith(\"category:\"),feature_names))[0]\n",
    "\n",
    "data_cats = df.iloc[:,categorical_columns]\n",
    "data_attrs = df.iloc[:,attribute_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "обучающая выборка: 4297\n",
      "контрольная выборка: 4297\n"
     ]
    }
   ],
   "source": [
    "#поделим выборку\n",
    "train_cats,test_cats,train_attrs,test_attrs = train_test_split(data_cats,data_attrs,test_size=0.5,random_state=42)\n",
    "print \"обучающая выборка:\" ,len(train_attrs)\n",
    "print \"контрольная выборка:\", len(test_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem definition\n",
    "\n",
    "Обучатор состоит из 2 частей\n",
    "* \"среды обучения\"\n",
    "* определятора полезностей\n",
    "* самой нейронки\n",
    "\n",
    "Среда обучения - такая штука, которая \n",
    "* подсовывает нейронке людей, \n",
    " * каждый раз выбирается случайное подмножество людей \n",
    "   * (можно и 100 и 10 - просто не успел сравнить)\n",
    "   * возвращает ответы на действия нейронки, попутно обновляя своё состояние\n",
    "Определятор полезностей\n",
    "* умеет считать полезность каждого действия по паре \"состояние СРЕДЫ, действие агента ИЗ этого состояния\"\n",
    "* имеет функции для быстрого определения наград за все действия в сессиях\n",
    "\n",
    "Нейронка будет далее по списку\n",
    "\n",
    "\n",
    "__***для простоты, CategoryGuess будет имплементировать И среду И награждатор (см. класс ниже)***__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from objective import BaseObjective\n",
    "from environment import BaseEnvironment\n",
    "\n",
    "class CategoryGuess(BaseObjective,BaseEnvironment):\n",
    "    def __init__(self,n_attrs,n_categories,batch_size=10):\n",
    "        self._attributes = _shared(\"X_attrs_data\",np.zeros([batch_size,n_attrs]),'uint8')\n",
    "        self._categories =  _shared(\"categories_data\",np.zeros([batch_size,n_categories]),'uint8')\n",
    "        self._batch_size = _shared(\"batch_size_scalar\",batch_size,'int32')\n",
    "        \n",
    "        _end_action = T.zeros([self._batch_size,1], dtype='uint8')\n",
    "        \n",
    "        self._joint_data = T.concatenate([self._attributes,\n",
    "                                          self._categories,\n",
    "                                          _end_action,\n",
    "                                         ],axis=1\n",
    ").astype('float32') #it is highly important to make sure \n",
    "        #that variables touched by theano.grad have float* type\n",
    "    \n",
    "        #indices\n",
    "        self._category_action_ids = T.arange(\n",
    "            self._attributes.shape[1],\n",
    "            self._attributes.shape[1]+self._categories.shape[1]\n",
    "        )\n",
    "        \n",
    "        self._end_action_id = self._joint_data.shape[1]-1\n",
    "    def load_data_batch(self,attrs_batch,categories_batch):\n",
    "        attrs_batch = np.array(attrs_batch)\n",
    "        categories_batch = np.array(categories_batch)\n",
    "        set_shared(self._attributes,attrs_batch)\n",
    "        set_shared(self._categories,categories_batch)\n",
    "        set_shared(self._batch_size,attrs_batch.shape[0])\n",
    "\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return int(self._joint_data.shape[1].eval())\n",
    "    @property\n",
    "    def observation_size(self):\n",
    "        return int((self._joint_data.shape[1]+1).eval())\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return int(self._joint_data.shape[1].eval())\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return int(self._batch_size.get_value())\n",
    "        \n",
    "    \n",
    "    def get_action_results(self,last_state,action,time_i):\n",
    "        \n",
    "        #state is a boolean vector: whether or not i-th action\n",
    "        #was tried already during this session\n",
    "        #last output[:,end_code] always remains 1 after first being triggered\n",
    "        \n",
    "        \n",
    "        batch_range = T.arange(action.shape[0])\n",
    "\n",
    "        can_do_action = T.eq(last_state[:,self._end_action_id],0)\n",
    "        \n",
    "        state_after_action = T.set_subtensor(last_state[batch_range,action],1)\n",
    "        \n",
    "        new_state = T.switch(\n",
    "            can_do_action.reshape([-1,1]),\n",
    "            state_after_action,\n",
    "            last_state\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        observation = T.concatenate([\n",
    "                self._joint_data[batch_range,action,None],#uint8[batch,1]\n",
    "                T.extra_ops.to_one_hot(action,self._joint_data.shape[1]),\n",
    "            ],axis=1)\n",
    "        \n",
    "        return new_state, observation\n",
    "\n",
    "    def get_reward(self,session_states,session_actions,batch_i):\n",
    "        \"\"\"\n",
    "        WARNING! this runs on a single session, not on a batch\n",
    "        reward given for taking the action in current environment state\n",
    "        arguments:\n",
    "            session_states float[time_i, memory_id]: environment state before taking action\n",
    "            session_actions int[time_i]: agent action at this tick\n",
    "        returns:\n",
    "            reward float[time_i]: reward for taking action from the given state\n",
    "        \"\"\"\n",
    "        time_range = T.arange(session_actions.shape[0])\n",
    "        \n",
    "\n",
    "        _has_tried_already = session_states[time_range,session_actions]\n",
    "        _session_is_active = T.eq(session_states[:,self._end_action_id],0)\n",
    "        \n",
    "        _has_finished_now = T.eq(session_actions,self._end_action_id)\n",
    "        _has_finished_now = T.set_subtensor(_has_finished_now[-1],1)\n",
    "        \n",
    "        _action_is_categorical = _in1d(session_actions, self._category_action_ids)\n",
    "        \n",
    "        _at_least_one_categorical_action = T.any(_action_is_categorical[:-1])\n",
    "        \n",
    "        _response = self._joint_data[batch_i,session_actions].ravel()\n",
    "        \n",
    "        #categorical and attributes\n",
    "        _reward_for_intermediate_action = T.switch(\n",
    "            _action_is_categorical,\n",
    "            _response*3-1,\n",
    "            _response*2-1\n",
    "        )\n",
    "        \n",
    "        #ending session\n",
    "        _reward_for_end_action = T.switch(_at_least_one_categorical_action, #if chosen at least 1 category\n",
    "                                          0,   #do not penalize\n",
    "                                          -3.)  #else punish\n",
    "        \n",
    "        #include end action\n",
    "        _reward_for_action = T.switch(\n",
    "            _has_finished_now,\n",
    "            _reward_for_end_action,\n",
    "            _reward_for_intermediate_action,\n",
    "        )\n",
    "        \n",
    "        _reward_if_first_time = T.switch(\n",
    "                _has_tried_already,\n",
    "                0,\n",
    "                _reward_for_action,\n",
    "            )\n",
    "        \n",
    "        _final_reward = T.switch(\n",
    "            _session_is_active,\n",
    "            _reward_if_first_time,\n",
    "            0,\n",
    "\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "        return _final_reward.astype(floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = CategoryGuess(train_attrs.shape[1],train_cats.shape[1],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "В свою очередь, нейронка состоит из 3 частей\n",
    "* память агента\n",
    "    * на вход получает пару (предыдущее состояние памятиа агента, сенсорная инфа о среде на ЭТОМ шагу)\n",
    "    * определяет текущее состояние памяти агента (на том же шагу, что и сенсорная инф-ция со входа)\n",
    "    * тут - GRUlayer, в потенциале - вообще что угодно, хоть 10 шагов LSTM за 1 раз\n",
    "       * какой именно GRUlayer:\n",
    "       * Reset gate - dense w/ expit\n",
    "       * Update gate - dense w/ expit\n",
    "       * Hidden update gate - dense w/ tanh\n",
    "\n",
    "* определятор Q-значений\n",
    "    * на вход получает память агента\n",
    "    * определяет Q_значения каждого действия\n",
    "    * тут - DenseLayer, потенциал - что угодно\n",
    "* приниматор решений по Q-значениям\n",
    "    * на вход получает Q-значения для действий\n",
    "    * определяет, как вывод нейронки трансформируется в решение,\n",
    "    * формат выхода - целые числа - номера выбранных действий\n",
    "     * тут - с вероятностью epsilon берётся решение, \n",
    "       * которому предсказана наибольшая полезность\n",
    "       * иначе берётся случайное равновероятное\n",
    "     * альтернативный вариант - решения выбираются с веростностями\n",
    "       * $Pi(a) = softmax(Qi(a)) = {e^{Qi(x)}} / {\\sum\\limits_{j=1}^n e^{Qj(X)}}$ \n",
    "       * где Q(a) - предсказанная полезность действия a\n",
    "* всё это делается векторизованно для батча произвольной размерности\n",
    "\n",
    "\n",
    "### Agent - просто обёртка над этими 3 пунктами\n",
    "* строго говоря, можно воткнуть любую нейронку, которая мапает \n",
    "* `float last_agent_state[batch,units], float observation[batch,sensors] ->`\n",
    "* `-> float new_agent_state[batch,units], float qvalues[batch,actions], int actions[batch]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resolver import EpsilonGreedyResolver\n",
    "from memory import GRUMemoryLayer\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "__idea__=\"\"\"\n",
    " \n",
    "We train two memory states:\n",
    "    first one, based on observations, and second one, based on first one;\n",
    "\n",
    "Note that here we update the second memory layer based on the CURRENT state\n",
    "of the first one. Instead, you can try to feed it with a previous state.\n",
    "\n",
    "The q-values are estimated on a concatenated state, effectively on both memory\n",
    "states together, but there is no problem with limiting q-evaluator to only one:\n",
    "just pass the correct gru layer as an incoming layer to the q-evaluator.\n",
    "\n",
    "Implementation:\n",
    "We concatenate both memories into 1 state to pass it through the session loop.\n",
    "\n",
    "To perform memory update, we need to slice the concatenated state back into\n",
    "two memory states.\n",
    "\n",
    "We do so by defining an input map function and passing it into agent.\n",
    "\n",
    "We than concatenate two new states back to form a new memory state.\n",
    "\n",
    "\"\"\"\n",
    "n_hid_1=256 #first GRU memory\n",
    "n_hid_2=128 #second GRU memory\n",
    "n_hid_3=128 #second GRU memory\n",
    "\n",
    "\n",
    "_observation_layer = lasagne.layers.InputLayer([None,env.observation_size],name=\"obs_input\")\n",
    "\n",
    "_prev_gru1_layer = lasagne.layers.InputLayer([None,n_hid_1],name=\"prev_gru1_state_input\")\n",
    "_prev_gru2_layer = lasagne.layers.InputLayer([None,n_hid_2],name=\"prev_gru2_state_input\")\n",
    "_prev_gru3_layer = lasagne.layers.InputLayer([None,n_hid_3],name=\"prev_gru3_state_input\")\n",
    "\n",
    "#memory\n",
    "gru1 = GRUMemoryLayer(n_hid_1,\n",
    "                     _observation_layer,\n",
    "                     _prev_gru1_layer,\n",
    "                     name=\"gru1\")\n",
    "\n",
    "gru2 = GRUMemoryLayer(n_hid_2,\n",
    "                     gru1,        #note that it takes CURRENT gru1 output as input.\n",
    "                                  #replacing that with _prev_gru1_state would imply taking previous one.\n",
    "                     _prev_gru2_layer,\n",
    "                     name=\"gru2\")\n",
    "\n",
    "gru3 = GRUMemoryLayer(n_hid_3,\n",
    "                     gru2,        #note that it takes CURRENT gru2 output as input.\n",
    "                                  #replacing that with _prev_gru2_state would imply taking previous one.\n",
    "                     _prev_gru3_layer,\n",
    "                     name=\"gru3\")\n",
    "\n",
    "\n",
    "concatenated_memory = lasagne.layers.concat([gru1,gru2,gru3])\n",
    "\n",
    "#q_eval\n",
    "q_eval = lasagne.layers.DenseLayer(concatenated_memory, #taking both memories. \n",
    "                                                        #Replacing with gru1 or gru2 would mean taking one\n",
    "                                   num_units = env.n_actions,\n",
    "                                   nonlinearity=lasagne.nonlinearities.linear,name=\"QEvaluator\")\n",
    "#resolver\n",
    "epsilon = _shared(\"e-greedy.epsilon\",0.9,dtype=floatX)\n",
    "\n",
    "resolver = EpsilonGreedyResolver(q_eval,epsilon=epsilon,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "#we need to define the new input map because concatenated_memory is a ConcatLayer and does not have default one\n",
    "\n",
    "def custom_input_map(last_hidden,observation):\n",
    "    \"\"\"just a function that maps memory states to respective inputs\"\"\"\n",
    "    return {\n",
    "        _prev_gru1_layer:last_hidden[:,0:n_hid_1],\n",
    "        _prev_gru2_layer:last_hidden[:,n_hid_1:n_hid_1+n_hid_2],\n",
    "        _prev_gru3_layer:last_hidden[:,n_hid_1+n_hid_2:n_hid_1+n_hid_2+n_hid_3],\n",
    "        _observation_layer:observation\n",
    "    }\n",
    "\n",
    "#all together\n",
    "agent = Agent(concatenated_memory,q_eval,resolver,input_map=custom_input_map\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = agent.get_sessions(env,session_length=20,\n",
    "                       batch_size=env._joint_data.shape[0],)\n",
    "state_seq,observation_seq,hidden_seq,qvalues_seq,action_seq = history\n",
    "\n",
    "rewards_seq = env.get_reward_sequences(state_seq,action_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_history = theano.function([],history+(rewards_seq,),mode=theano.compile.mode.FAST_RUN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weights\n",
    "* вот так нахаляву можно вытащить ВСЕ веса нейронок агента\n",
    "* что НЕ вытаскивается \n",
    " * архитектура сети\n",
    " * гиперпараметры (жадность, gamma)\n",
    " * состояния среды/наблюдения\n",
    "* короче, вытаскиваются только \"нейронячие\" параметры\n",
    "* хочешь, чтобы в твоём слое вытащилась какая-то shared переменная - добавь её в параметры слоя [с каким-ть тэгом, если хочешь]\n",
    "* всё как в ванильной лазанье"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gru1.W_in_to_updategate,\n",
       " gru1.W_hid_to_updategate,\n",
       " gru1.b_updategate,\n",
       " gru1.W_in_to_resetgate,\n",
       " gru1.W_hid_to_resetgate,\n",
       " gru1.b_resetgate,\n",
       " gru1.W_in_to_hidden_update,\n",
       " gru1.W_hid_to_hidden_update,\n",
       " gru1.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " gru3.W_in_to_updategate,\n",
       " gru3.W_hid_to_updategate,\n",
       " gru3.b_updategate,\n",
       " gru3.W_in_to_resetgate,\n",
       " gru3.W_hid_to_resetgate,\n",
       " gru3.b_resetgate,\n",
       " gru3.W_in_to_hidden_update,\n",
       " gru3.W_hid_to_hidden_update,\n",
       " gru3.b_hidden_update,\n",
       " QEvaluator.W,\n",
       " QEvaluator.b]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all trainable weights\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gru1.W_in_to_updategate,\n",
       " gru1.W_hid_to_updategate,\n",
       " gru1.b_updategate,\n",
       " gru1.W_in_to_resetgate,\n",
       " gru1.W_hid_to_resetgate,\n",
       " gru1.b_resetgate,\n",
       " gru1.W_in_to_hidden_update,\n",
       " gru1.W_hid_to_hidden_update,\n",
       " gru1.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " gru3.W_in_to_updategate,\n",
       " gru3.W_hid_to_updategate,\n",
       " gru3.b_updategate,\n",
       " gru3.W_in_to_resetgate,\n",
       " gru3.W_hid_to_resetgate,\n",
       " gru3.b_resetgate,\n",
       " gru3.W_in_to_hidden_update,\n",
       " gru3.W_hid_to_hidden_update,\n",
       " gru3.b_hidden_update]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all weights but for Qvalues prediciton ones (not to penalize q-predictors by l2)\n",
    "weights_inner = filter(lambda w: \"QEvaluator\" not in w.name,weights)\n",
    "weights_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# целевая функция для обучения сети\n",
    "\n",
    "* Интуитивно, цель - предсказывать Q-значения (полезности) действий\n",
    "* $Q(s,a) = R(s,a) + gamma* Max_{a'}{ Q(s',a')}  $\n",
    " * R(s,a) - непосредственная награда за действие (+5 если угадал, -1 если нет и т.п.)\n",
    " * s' - состояние, в которое агент попал, сделав действие a из состояния s\n",
    " * a' - действие, которое можно совершить в состоянии s'\n",
    " * gamma - во сколько раз награда/пинок через 1 шаг менее значимы, чем сейчас\n",
    " * интуиция - \"в состоянии s полезность действия a - это непосредственная печенька за это действие ПЛЮС все печеньки, которые можно собрать, если после этого действия вести себя оптимально\"\n",
    " \n",
    "* В реальном мире оптимизируются попарно 2 приближения этой функции\n",
    "* $ RMSE_{reccurent}: (NN_s(a) - (R + gamma* Max_a'{ NN_s'(a')})^2$\n",
    " * NN_s(a) предсказаннная из состояния s полезност Q(s,a)\n",
    " * считается для тех NN_s(a), которые агент совершил\n",
    " * NN_s'(a') - полезности всех действий сразу после совершения a из состяния s\n",
    "\n",
    "* $ RMSE_{naive}: (NN_s(a^*) - (R + gamma*  NN_s'(a'^*))^2$\n",
    " * аналогично предыдущему, но a* - реально совершённое агентом действие\n",
    " * оценка смещена, но куда проще оптимизируется, чем предыдущая\n",
    "   * по крайней мере, с softmax принятием решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get pairs of corresponding [predicted Qvalues, reference Qvalues] to train on\n",
    "\n",
    "is_alive_seq = T.eq(state_seq[:,:,-1],0)\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = _shared('q_learning_gamma',np.float32(0.95),floatX)\n",
    "\n",
    "ref_tuples = env.get_reference(qvalues_seq,action_seq,rewards_seq,is_alive_seq,gamma_or_gammas=gamma,)\n",
    "\n",
    "action_Qvalues, predicted_Qvalues, is_end = ref_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate \"alive\" ticks from all sessions to then compute errors\n",
    "\n",
    "to_be_raveled = [action_seq,rewards_seq,action_Qvalues, predicted_Qvalues, is_end]\n",
    "\n",
    "raveled_tuples = env.ravel_alive(is_alive_seq,*to_be_raveled)\n",
    "\n",
    "action_ids_ravel,immediate_rewards_ravel,action_Qvalues_ravel,\\\n",
    "reference_Qvalues_ravel,is_end_ravel= raveled_tuples\n",
    "\n",
    "\n",
    "\n",
    "from auxilary import consider_constant\n",
    "reference_Qvalues_ravel = consider_constant(reference_Qvalues_ravel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_recurrent = lasagne.objectives.squared_error(reference_Qvalues_ravel,action_Qvalues_ravel).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_ravel = T.concatenate([w.ravel() for w in weights_inner])\n",
    "reg_l2 = reg_l2 = T.mean(weight_ravel**2)*0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = loss_recurrent + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# алгоритм обучения\n",
    "* обучаются все параметры обычным Stochastic Gradient Descent с Nestrov Mommentum\n",
    "* регуляризации нет, ибо пока не понадобилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_updates = lasagne.updates.nesterov_momentum(loss,weights,learning_rate=0.01,momentum=0.9)\n",
    "#updates = lasagne.updates.sgd(loss_recurrent,weights,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = immediate_rewards_ravel.sum()/ rewards_seq.shape[0]\n",
    "train_fun = theano.function([],[loss,loss_recurrent,reg_l2,mean_session_reward],updates=_updates)\n",
    "loss_fun = theano.function([],[loss,loss_recurrent,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation\n",
    "* удобная [мне одному] функция, которая показывает примеры сессий и внутреннее состояние нейронки\n",
    "* текстовая часть - \n",
    "    * #(номер_действия) (имя_действия) (предсказанное_Qvalue) -> (награда) | следующая итерация\n",
    "* картинка (если display = True)\n",
    "    * X - моменты времени, Y - попугаи\n",
    "    * жЫрные линии - Q-значения категорий\n",
    "    * тонкие линии - Q-значения узнавания аттрибутов\n",
    "    * точки на линиях - выбранные действия\n",
    "    * пунктирные линии - активация нейронов в памяти агента\n",
    "    * синяя черта (тонкая, вертикальная) - конец сессии (действие или лимит времени)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation_fun = theano.function([],[action_ids_ravel,action_Qvalues_ravel,immediate_rewards_ravel,\n",
    "                                     reference_Qvalues_ravel,is_end_ravel,\n",
    "                                     hidden_seq,qvalues_seq,action_seq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new names order\n",
    "feature_names = list(data_attrs.columns) + list(data_cats.columns) + [\"<end_session_action>\"]\n",
    "is_categorical = lambda i: feature_names[i].startswith(\"category:\")\n",
    "\n",
    "def print_session(n_sessions = 3,display=False,legend=True):\n",
    "    \n",
    "    a_lot_of_data = evaluation_fun()    \n",
    "    actions,qvalues,rewards,ref_qvalues,end_sel = a_lot_of_data[:5]\n",
    "    hidden_log,qvalues_log,action_log = a_lot_of_data[5:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    b_i = 0 #batch iter ~ session iter\n",
    "    t_i = 0 #time iter\n",
    "    \n",
    "\n",
    "    for a,q,r,q_ref,is_end in zip(actions,qvalues,rewards,ref_qvalues,end_sel):\n",
    "        print \"#%i: %s (%.3f) -> %s |\"%(a,feature_names[a],q, r,),\n",
    "        \n",
    "        \n",
    "        t_i +=1\n",
    "        if is_end ==1:\n",
    "            \n",
    "            print '<end>'\n",
    "            print 'true:',\n",
    "            for cat in env._categories.get_value()[b_i].nonzero()[0]:\n",
    "                print feature_names[cat+env._attributes.shape[1].eval()],\n",
    "            print\n",
    "                \n",
    "            #plot qvalues\n",
    "            if display:\n",
    "                plt.figure(figsize=[16,8])\n",
    "\n",
    "                q_values = qvalues_log[b_i].T\n",
    "                for i in range(q_values.shape[0]):\n",
    "                    plt.plot(q_values[i],label=feature_names[i],linewidth = 1 + 4*is_categorical(i))\n",
    "\n",
    "                hidden_activity =  hidden_log[b_i].T\n",
    "                for i, hh in enumerate(hidden_activity):\n",
    "                    plt.plot(hh,'--',label='n'+str(i))\n",
    "                    \n",
    "                session_actions = action_log[b_i,:t_i]\n",
    "                action_range = np.arange(len(session_actions))\n",
    "                plt.scatter(action_range, qvalues_log[b_i][action_range, session_actions])\n",
    "\n",
    "\n",
    "                #session end\n",
    "                plt.plot(np.repeat(t_i-1,2),np.linspace(-0.5,0.5,2))\n",
    "                plt.xlim(0,max(t_i*1.1,2))\n",
    "                plt.xticks(np.arange(t_i))\n",
    "                plt.grid()\n",
    "                if legend:\n",
    "                    plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "            print\n",
    "            #/\n",
    "            b_i +=1\n",
    "            t_i = 0\n",
    "\n",
    "            if b_i >= n_sessions:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_random_batch(env,attrs,cats,batch_size=10):\n",
    "    \n",
    "    attrs,cats = np.array(attrs),np.array(cats)\n",
    "    \n",
    "    assert len(attrs) == len(cats)\n",
    "    batch_ids = np.random.randint(0,len(attrs),batch_size)\n",
    "    env.load_data_batch(attrs[batch_ids],cats[batch_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "epoch_counter = 1\n",
    "score_log = defaultdict(dict) #{metric_name: {epoch:value} }\n",
    "\n",
    "def plot_scores(title=\"metrics\"):\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.title(\"metrics history\")\n",
    "    for metric_name, metric_dict in score_log.items():            \n",
    "        plt.plot(*zip(*sorted(metric_dict.items())),label = metric_name)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "ma_reward = 0.\n",
    "ma_reward_greedy = 0.\n",
    "ma_reward_test = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# само обучение\n",
    "* шаг алгоритма - \n",
    " * выбрать B случайных людей\n",
    " * пересчитать \"жадность\" алгоритма (чем дальше, тем более жадный)\n",
    " * сделать шаг обучения\n",
    " * раз в M итераций - добавить точку на график обучения по 3 метрикам\n",
    "   * ожидание награды за сессию при текущей жадности на тренировочных данных\n",
    "   * то же самое при максимальной жадности\n",
    " * раз в N итераций - \n",
    "   * показать несколько примеров сессий на обучающих данных при разной жадности\n",
    "   * показать графики обучения\n",
    "* раз в K итераций - записать веса нейронки в файл (snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 875100,loss 0.28852, greed 0.95000, rewards: ( train -13.33560, det_train 0.00000) \n",
      "rec 0.287 reg 0.001\n",
      "epoch 875200,loss 0.45257, greed 0.95000, rewards: ( train 5.16904, det_train 0.33000) "
     ]
    }
   ],
   "source": [
    "n_epochs = 10**6\n",
    "batch_size=10\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    resolver.rng.seed(epoch_counter)\n",
    "    \n",
    "    creeping_greed = min(0.1 + (1.-np.exp(-epoch_counter/100000.)*0.9),0.95)\n",
    "    creeping_epsilon = 1.-creeping_greed\n",
    "    \n",
    "    \n",
    "    #train\n",
    "    load_random_batch(env,train_attrs,train_cats,batch_size=batch_size)\n",
    "    \n",
    "    loss,q_loss,reg_loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #reward loss moving average\n",
    "    ma_reward = alpha*avg_reward+ (1-alpha)*ma_reward\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if epoch_counter%1000==0:\n",
    "        print \"Evaluation:\"\n",
    "        print '\\ndeterministic'\n",
    "        set_shared(resolver.epsilon,0)\n",
    "        print_session(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print '\\ncurrent_greediness (%f)'%creeping_greed\n",
    "        set_shared(resolver.epsilon,creeping_epsilon)\n",
    "        print_session(1)\n",
    "        \n",
    "        plot_scores(experiment_setup_name)\n",
    "\n",
    "    if epoch_counter% 25000 ==0:\n",
    "        save(resolver,\"/root/agentnet_snapshots/{}.epoch{}.pcl\".format(experiment_setup_name,epoch_counter))\n",
    "        print \"snapshot saved\"\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if epoch_counter%100 ==0:\n",
    "        print \"epoch %i,loss %.5f, greed %.5f, rewards: ( train %.5f, det_train %.5f) \"%(\n",
    "            epoch_counter,loss,creeping_greed,ma_reward,ma_reward_greedy)\n",
    "        print \"rec %.3f reg %.3f\"%(q_loss,reg_loss)\n",
    "        \n",
    "        score_log[\"in-training session avg reward\"][epoch_counter] = ma_reward\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        set_shared(resolver.epsilon,0)\n",
    "        avg_reward_greedy = loss_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"avg session reward greedy (on training)\"][epoch_counter] = ma_reward_greedy\n",
    "\n",
    "        #greedy test\n",
    "        set_shared(resolver.epsilon,0)\n",
    "        load_random_batch(env,test_attrs,test_cats,batch_size=batch_size)\n",
    "\n",
    "        avg_reward_test = loss_fun()[-1]\n",
    "        ma_reward_test = (1-alpha)*ma_reward_test + alpha*avg_reward_test\n",
    "        score_log[\"avg session reward greedy (on test)\"][epoch_counter] = ma_reward_test\n",
    "        \n",
    "\n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        set_shared(resolver.epsilon,creeping_epsilon)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_scores(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_random_batch(env,train_attrs,train_cats,10)\n",
    "print_session(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_session(display=True,legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_random_batch(env,test_attrs,test_cats,10)\n",
    "print_session(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_session(display=True,legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
