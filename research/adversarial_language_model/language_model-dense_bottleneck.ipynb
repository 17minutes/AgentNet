{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=gpu2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "#the problem is too simple to be run on GPU. Seriously.\n",
    "%env THEANO_FLAGS='device=gpu2'\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [in development] this is just a minimalistic language model so far\n",
    "# This tutorial explains the Generator agent type applied to conversation modelling\n",
    "* experiment setup\n",
    "* designing agent\n",
    "* computing losses\n",
    "* training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "with open(\"/home/jheuristic/yozhik/dl_s8/tokens.pcl\") as fin:\n",
    "    tokens = cPickle.load(fin)[0]\n",
    "\n",
    "titles = np.load(\"/home/jheuristic/yozhik/dl_s8/titles.npy\").astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL;асфальтно;экусплуатации;препятствующую;рекордсервис\n",
      "[[226421 488637 197907      0      0      0      0      0      0      0]\n",
      " [400357 375323      0      0      0      0      0      0      0      0]\n",
      " [467017 107402      0      0      0      0      0      0      0      0]]\n",
      "toyota sera 1991 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "монтаж кровли NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "костюм steilmann NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "ford focus 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "турбина 3 0 bar NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "ваз 2115 samara 2005 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "ваз 2105 1998 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "комната 12 м² в 3 к 3 14 эт NULL NULL NULL NULL NULL NULL\n",
      "дом 42 м² на участке 7 сот NULL NULL NULL NULL NULL NULL NULL NULL\n",
      "подголовники ваз 2109 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL\n"
     ]
    }
   ],
   "source": [
    "print ';'.join(tokens[:5])\n",
    "print titles[:3,:10]\n",
    "for title in titles[:10]:\n",
    "    print ' '.join(map(tokens.__getitem__,title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation has to contain three parts:\n",
    " * Memory layer(s)\n",
    "  * in this case, a single one-step GRU\n",
    " * Q-values evaluation layers\n",
    "  * in this case, a lasagne dense layer based on memory layer\n",
    " * Resolver - acton picker layer\n",
    "  * in this case, the resolver has epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.memory import GRUMemoryLayer\n",
    "from agentnet.agent import Generator\n",
    "from agentnet.resolver import ProbablisticResolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import lasagne\n",
    "n_hid_1=2048 #first GRU memory\n",
    "n_hid_2=2048 #second GRU memory\n",
    "embedding_size=256\n",
    "vocab_size = len(tokens) #number of words in dictionary\n",
    "\n",
    "\n",
    "\n",
    "_observation_layer = lasagne.layers.InputLayer((None,),name=\"obs_input\")\n",
    "\n",
    "_prev_gru1_layer = lasagne.layers.InputLayer((None,n_hid_1),name=\"prev_gru1_state_input\")\n",
    "_prev_gru2_layer = lasagne.layers.InputLayer((None,n_hid_2),name=\"prev_gru2_state_input\")\n",
    "\n",
    "\n",
    "emb = lasagne.layers.EmbeddingLayer(_observation_layer,vocab_size,embedding_size,name=\"token_vectors\")\n",
    "\n",
    "#memory\n",
    "gru1 = GRUMemoryLayer(n_hid_1,\n",
    "                     emb,\n",
    "                     _prev_gru1_layer,\n",
    "                     name=\"gru1\")\n",
    "\n",
    "gru2 = GRUMemoryLayer(n_hid_2,\n",
    "                     gru1,        #note that it takes CURRENT gru1 output as input.\n",
    "                                  #replacing that with _prev_gru1_state would imply taking previous one.\n",
    "                     _prev_gru2_layer,\n",
    "                     name=\"gru2\")\n",
    "\n",
    "concatenated_memory = lasagne.layers.concat([gru1,gru2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#policy\n",
    "\n",
    "\n",
    "greed = theano.shared(np.float32(1),\"prob_multiplier\")\n",
    "\n",
    "bottleneck_layer = lasagne.layers.DenseLayer(concatenated_memory,\n",
    "                                            num_units = embedding_size,\n",
    "                                            nonlinearity = lasagne.nonlinearities.tanh,\n",
    "                                            name = \"pre-output dense layer (bottleneck)\")\n",
    "\n",
    "\n",
    "policy_layer = lasagne.layers.DenseLayer(bottleneck_layer, #taking both memories. \n",
    "                                                        #Replacing with gru1 or gru2 would mean taking one\n",
    "                                         num_units = vocab_size,\n",
    "                                         nonlinearity=lambda x: lasagne.nonlinearities.softmax(x*greed),\n",
    "                                         name=\"policy_original\")\n",
    "\n",
    "#resolver\n",
    "\n",
    "\n",
    "resolver = ProbablisticResolver(policy_layer,assume_normalized=True,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#we need to define the new input map because concatenated_memory is a ConcatLayer and does not have default one\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "#all together\n",
    "agent = Generator(\n",
    "    _observation_layer,\n",
    "    OrderedDict([\n",
    "            (gru1,_prev_gru1_layer),\n",
    "            (gru2,_prev_gru2_layer)]),\n",
    "     policy_layer,\n",
    "     resolver\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[token_vectors.W,\n",
       " gru1.W_in_to_updategate,\n",
       " gru1.W_hid_to_updategate,\n",
       " gru1.b_updategate,\n",
       " gru1.W_in_to_resetgate,\n",
       " gru1.W_hid_to_resetgate,\n",
       " gru1.b_resetgate,\n",
       " gru1.W_in_to_hidden_update,\n",
       " gru1.W_hid_to_hidden_update,\n",
       " gru1.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " pre-output dense layer (bottleneck).W,\n",
       " pre-output dense layer (bottleneck).b,\n",
       " policy_original.W,\n",
       " policy_original.b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * One-step recurrent layer\n",
    "     * takes input and one's previous state\n",
    "     * returns new memory state\n",
    "   * Can be arbitrary lasagne layer\n",
    "   * Several one-step recurrent units are implemented in __agentnet.memory__\n",
    "   * Note that lasagne's default recurrent networks roll for several steps at once\n",
    "     * in other words, __using lasagne recurrent units as memory means recurrence inside recurrence__\n",
    " * Using more than one memory layer is explained in farther tutorials\n",
    "\n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are represented as tensors with dimensions matching pattern [batch_session_i, time_tick, ...]\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = titles.shape[1]\n",
    "\n",
    "sequences_batch = theano.shared(np.zeros([3,seq_length],dtype=\"int32\"),name=\"reference_sequences\")\n",
    "\n",
    "batch_size = sequences_batch.shape[0]\n",
    "\n",
    "\n",
    "#have to create integer initial_actions to send them to .get_sessions as initial observations\n",
    "#if default observations='zeros' are sent instead, BaseAgent generates floats instead of integers\n",
    "initial_actions = [T.zeros((batch_size,),dtype='int32')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = agent.get_sessions(session_length=seq_length,\n",
    "                             recorded_sequences=sequences_batch,\n",
    "                             batch_size=batch_size,\n",
    "                             initial_actions = initial_actions)\n",
    "\n",
    "env_states,observation_seq,agent_states,action_seq,policy_seq = history\n",
    "\n",
    "\n",
    "gru1_seq = agent_states[gru1]\n",
    "gru2_seq = hidden_seq = agent_states[gru2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this case, we want to \n",
    " * first get pairs of (predicted Qvalue, reference Qvalue) for all actions commited\n",
    " * second, define loss function\n",
    " * third, compute grad and update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_probas = policy_seq.reshape([-1,vocab_size])\n",
    "predicted_probas = T.maximum(predicted_probas,1e-5)\n",
    "\n",
    "model_loss = lasagne.objectives.categorical_crossentropy(predicted_probas,\n",
    "                                                         sequences_batch.ravel()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = model_loss + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,model_loss,reg_l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_sequences = theano.function([],action_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0\tfull:15.58874\tllh:11.51293\treg:4.07582\n"
     ]
    }
   ],
   "source": [
    "loss_seq = []\n",
    "for i in range(50000):\n",
    "    new_batch = titles[np.random.randint(len(titles),size=10)]\n",
    "    sequences_batch.set_value(new_batch)\n",
    "    \n",
    "    loss_seq.append(train_fun())\n",
    "    \n",
    "    if i % 100==0:\n",
    "        quality = \"iter:%i\\tfull:%.5f\\tllh:%.5f\\treg:%.5f\"%tuple([i]+map(float,evaluation_fun()))        \n",
    "        print quality\n",
    "        log+=quality+'\\n'\n",
    "        \n",
    "        examples = get_sequences()[:3]\n",
    "        for tid_line in examples:\n",
    "            line = ' '.join(map(tokens.__getitem__,tid_line))\n",
    "            print line\n",
    "            log += line+'\\n'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print log[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save\n",
    "save(resolver,\"./lm-dense.pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
