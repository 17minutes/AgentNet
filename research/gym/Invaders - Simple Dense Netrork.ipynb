{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"tutorial.gym.atari.spaceinvaders-v0.cnn\"\n",
    "snapshot_path = \".\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'SpaceInvaders-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 10\n",
    "\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is my machine-specific config. Comment out if you are not me.\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "snapshot_path = \"/home/jheuristic/yozhik/agentnet_snapshots/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use AgentNet for OpenAI Gym environments\n",
    "\n",
    "* We take Space Invadets game as an example\n",
    "* We train a simple-stupid convolutional network for Q_learning objective\n",
    "* We do have a GPU for that, so if you don't, we have a \"linear regression\" mode - just uncomment it :)\n",
    "* We train via experience replay using SessionPoolEnvironment, explained below\n",
    "* We do NOT use recurrent layers for simplicity\n",
    "* We do NOT use \"smart\" experience replay for simplicity\n",
    "* We use simple stupid one-step q-learning.. you guessed it, for simplicity\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose your side\n",
    "\n",
    "#Sith\n",
    "#mode = \"simple convnet\"\n",
    "\n",
    "#Jedi\n",
    "mode = \"dense nn\"\n",
    "\n",
    "#Stormtrooper\n",
    "#mode = \"linear regression\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print action_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "  \n",
    "### Simple no-memory model\n",
    "\n",
    "Since we have almost fully observable environment AND we want to keep baseline simple, we shall use NO agent memories and a simple lasagne CNN to process observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.memory import WindowAugmentation\n",
    "from agentnet.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "#image observation\n",
    "observation_layer = lasagne.layers.InputLayer(observation_shape,\n",
    "                                                    name=\"images input\")\n",
    "\n",
    "observation_reshape = lasagne.layers.dimshuffle(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n",
    "\n",
    "#convolution\n",
    "if mode == 'simple convnet':\n",
    "    print \"Using some CNNs\"\n",
    "    cnn = lasagne.layers.Conv2DLayer(observation_reshape,num_filters=32,filter_size=(5,5),name='cnn0')\n",
    "    cnn = lasagne.layers.MaxPool2DLayer(cnn,(5,5), name='pool0')\n",
    "    cnn = lasagne.layers.Conv2DLayer(cnn,num_filters=64,filter_size=(5,5),name='cnn1')\n",
    "    cnn = lasagne.layers.MaxPool2DLayer(cnn,(5,5), name='pool1')\n",
    "    dnn = lasagne.layers.DropoutLayer(cnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "    dnn = lasagne.layers.DenseLayer(dnn,num_units=500,name='dense1')\n",
    "    nn = dnn\n",
    "#dense with dropout\n",
    "elif mode == \"dense nn\":\n",
    "    print \"Using simple dense network\"\n",
    "    \n",
    "    dnn = lasagne.layers.DenseLayer(observation_reshape,num_units=500,name='dense0')\n",
    "    dnn = lasagne.layers.DropoutLayer(dnn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "    dnn = lasagne.layers.DenseLayer(dnn,num_units=500,name='dense1')\n",
    "    nn = dnn\n",
    "\n",
    "#linear regression\n",
    "else:\n",
    "    print \"Using linear model\"\n",
    "    nn = observation_reshape\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "flat_nn = lasagne.layers.FlattenLayer(nn,outdim=2,name='flat frame output')\n",
    "\n",
    "\n",
    "#memory\n",
    "#using simple window-based memory that stores sevelar states\n",
    "#the environment does not need any more\n",
    "\n",
    "window_size = 3\n",
    "prev_window = lasagne.layers.InputLayer((None,window_size,flat_nn.output_shape[1]),\n",
    "                                        name = \"previous window state\")\n",
    "\n",
    "window = WindowAugmentation(flat_nn,prev_window,name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "memory_dict = {window:prev_window}\n",
    "\n",
    "\n",
    "#q_eval\n",
    "q_eval = lasagne.layers.DenseLayer(window,\n",
    "                                   num_units = n_actions,\n",
    "                                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                                   name=\"QEvaluator\")\n",
    "#resolver\n",
    "epsilon = theano.shared(np.float32(0.5),\"e-greedy.epsilon\")\n",
    "\n",
    "resolver = EpsilonGreedyResolver(q_eval,epsilon=epsilon,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * to be covered where they are more useful\n",
    " \n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* compute action and next state given observation and prev state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "applier_observation = T.tensor4(\"input image\",dtype=floatX)\n",
    "\n",
    "applier_window = T.tensor3(\"prev window\",dtype=floatX)\n",
    "\n",
    "\n",
    "res =agent.get_agent_reaction({window:applier_window},\n",
    "                              applier_observation,\n",
    "                              deterministic = True #disable dropout here. Only enable in experience replay\n",
    "                             )\n",
    "\n",
    "\n",
    "applier_actions,applier_new_states,applier_policy = res\n",
    "\n",
    "applier_fun = theano.function([applier_observation,applier_window],\n",
    "        applier_actions+applier_new_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a nice pythonic interface\n",
    "def step(observation, prev_memories = 'zeros'):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((N_PARALLEL_GAMES,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype=floatX) \n",
    "                         for mem in agent.state_variables]\n",
    "    \n",
    "    res = applier_fun(np.array(observation),prev_memories[0])\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action,memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A whole lot of space invaders\n",
    "ataries = [gym.make(GAME_TITLE) for i in range(N_PARALLEL_GAMES)]\n",
    "for atari in ataries:\n",
    "    atari.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interact(ataries,n_steps = 100,verbose=False):\n",
    "    \"\"\"generate interaction sessions with ataries (openAI gym atari environments)\n",
    "    Sessions will have length n_steps. \n",
    "    Each time one of games is finished, it is immediately getting reset\"\"\"\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = map(lambda atari: atari.render('rgb_array'), ataries)\n",
    "    \n",
    "    prev_memory_states = 'zeros'\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        actions,new_memory_states = step(prev_observations,prev_memory_states)\n",
    "\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = zip(*map(lambda atari, action: atari.step(action), \n",
    "                                                            ataries,actions))\n",
    "        \n",
    "        for i in range(len(ataries)):\n",
    "            if is_done[i]:\n",
    "                ataries[i].reset()\n",
    "                if verbose:\n",
    "                    print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,new_memory_states,is_done,infos))\n",
    "        \n",
    "        prev_observations = new_observations\n",
    "        prev_memory_states = new_memory_states\n",
    "                \n",
    "    \n",
    "    \n",
    "    return zip(*history_log)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "obsevation_log,action_log,reward_log,_,_,_  = interact(ataries,50)\n",
    "\n",
    "\n",
    "print np.array(reward_log)[:10].T\n",
    "print np.array(action_names)[np.array(action_log)[:3,:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "### we shall use session pool environment to immediately train on new sessions\n",
    "* theoretically, this environemnt is designed for storing a lot of game sessions and training on random batches,\n",
    "* but for the sake of baseline, it's a one-time usage pool\n",
    "\n",
    "1. Interact with ALE, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_pool(env, ataries,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    \n",
    "    obsevation_log,action_log,reward_log,_,is_done_log,_= interact(ataries,n_steps=n_steps)\n",
    "    \n",
    "    \n",
    "    #tensor dimensions\n",
    "    \n",
    "    # [batch_i, time_i, width, height, rgb]\n",
    "    observation_tensor = np.array(obsevation_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i,time_i]\n",
    "    action_tensor = np.array(action_log).swapaxes(0,1)\n",
    "    \n",
    "    # [batch_i, time_i]\n",
    "    reward_tensor = np.array(reward_log).swapaxes(0,1)\n",
    "\n",
    "    # [batch_i, time_i]\n",
    "    is_alive_tensor = 1- np.array(is_done_log,dtype = 'int8').swapaxes(0,1)\n",
    "    \n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,[])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,ataries,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#produce interaction sequences of length <= 10\n",
    "\n",
    "_,observation_seq,_,_,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "#observation seq are the observation tensor we just loaded\n",
    "#qvalues seq are agent's Qvalues obtained via experience replay\n",
    "\n",
    "\n",
    "#The three \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#second - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n",
    "\n",
    "#the actions agent took in the original recorded game\n",
    "action_seq = env.actions[0]\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.rewards\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.is_alive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.95),name = 'q_learning_gamma')\n",
    "\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                        action_seq,\n",
    "                                                        rewards_seq,\n",
    "                                                        is_alive_seq,\n",
    "                                                        gamma_or_gammas=gamma,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "mse_loss = elwise_mse_loss.sum() / is_alive_seq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = lasagne.updates.adadelta(loss,\n",
    "                                             weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,mse_loss,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session visualization tools\n",
    "\n",
    "### [warning, this thing basicly tries to track various Qvalues over time]\n",
    "### [but it's bulky and stupid, so don't try to understand it if it didn't come naturally]\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* usually useful to get insight on what worked and what din't\n",
    "* in this case, we print strings following pattern\n",
    "  * [action_name] ([predicted action qvalue]) -> reward [reference qvalue] | next iteration\n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are Qvalues for actions\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "__Warning! the visualization tools are underdeveloped and only allow simple operations.__\n",
    "\n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        qvalues_seq, action_seq,rewards_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False,max_n_sessions = 3,update = True):\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in ataries[:max_n_sessions]]\n",
    "    \n",
    "    if update:\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "    \n",
    "    \n",
    "    printables = get_printables()\n",
    "    \n",
    "    \n",
    "    for i in range(max_n_sessions):\n",
    "        plt.imshow(pictures[i])\n",
    "        plt.show()\n",
    "            \n",
    "        qvalues_log,actions_log,reward_log, is_alive_log = map(lambda v: np.array(v[i:i+1]), printables)\n",
    "        \n",
    "\n",
    "        print_sessions(qvalues_log,actions_log,reward_log,\n",
    "                       is_alive_seq = is_alive_log,\n",
    "                       action_names=action_names,\n",
    "                       legend = True, #do not show legend since there's too many labeled objects\n",
    "                      plot_policy = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "epsilon.set_value(0.05)\n",
    "display_sessions(with_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tools for model persistence (in progress now. Requires unique names)\n",
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 25000\n",
    "batch_size= 10\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,ataries,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/500.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%50 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,ataries,replay_seq_len)\n",
    "\n",
    "        print \"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy)\n",
    "        print \"rec %.3f reg %.3f\"%(q_loss,l2_penalty)\n",
    "\n",
    "    if epoch_counter %1000 ==0:\n",
    "        print \"Learning curves:\"\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print \"Random session examples\"\n",
    "        display_sessions(with_plots=False)\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %5000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print \"saved\", snap_name\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random session examples\"\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(ataries,n_steps = 100):\n",
    "\n",
    "    history_log = []\n",
    "        \n",
    "    prev_observations = map(lambda atari: atari.render('rgb_array'), ataries)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        #show pics\n",
    "        map(lambda atari: atari.render('human'), ataries)\n",
    "\n",
    "        \n",
    "        actions = applier_fun(prev_observations)[0]\n",
    "        \n",
    "        new_observations, cur_rewards, is_done, infos = zip(*map(lambda atari, action: atari.step(action), \n",
    "                                                            ataries,actions))\n",
    "        \n",
    "        for i in range(len(ataries)):\n",
    "            if is_done[i]:\n",
    "                ataries[i].reset()\n",
    "                print \"atari\",i,\"reloaded\"\n",
    "        \n",
    "        \n",
    "        #append observation -> action -> reward tuple\n",
    "        history_log.append((prev_observations,actions,cur_rewards,is_done))\n",
    "        prev_observations = new_observations\n",
    "                \n",
    "    \n",
    "    \n",
    "    return zip(*history_log)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "show(ataries[:1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
