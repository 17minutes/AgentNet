{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"research.rl_comparison.a3c\"\n",
    "snapshot_path = \"/home/jheuristic/yozhik/agentnet_snapshots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=cpu'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/thenv/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "#the problem is too simple to be run on GPU. Seriously.\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial builds above the basic tutorial and shows several advanced tools\n",
    "* multi-layer (and in principle, arbitrary) agent memory\n",
    "* different reinforcement learning algorithms\n",
    "* model persistence\n",
    "\n",
    "__[todo: add more]__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we load an experiment environment (description below)\n",
    "* Designing one from scratch is explained in later tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This experiment Wikipedia data sample on musicians, scraped via the scripts present in this directory. \n",
      "For each musician, we know a number of boolean features (factors) on various topics like \n",
      " * whether or not he/she was active in 1990's, 2000's, etc.\n",
      " * whether or not he/she plays guitar, piano, etc\n",
      " * whether or not he/she was born in 50's, 60's, etc.\n",
      " * what wikipedia categories does he/she belong to\n",
      " etc.\n",
      "\n",
      "\n",
      "\n",
      "In the initial moment of time, agent knows nothing about any of them.\n",
      "At each turn, agent may decide to\n",
      " - \"open\" one of the hidden factors.\n",
      "   - if the factor turns out to be 1, agent receives +3 reward for Wikipedia categories, +1 for other categories,\n",
      "   - Otherwise, the reward equals -1 for Wikipedia categories, -1 for other categores\n",
      "   - all these rewards are parameterisable during environment creation\n",
      " - decide to quit session\n",
      "   - yields reward of 0 and ends the interaction.\n",
      "   - all farther actions will have no effect until next session\n",
      "\n",
      "It is expected, that in order to maximize it's expected reward, the agent\n",
      "will converge to a strategy of polling several key features and then utilizing learned\n",
      "dependencies between these factors and other ones. For example, if a particular genre was\n",
      "popular during particular decays, it makes sense to poll genres and than \"open\" the corresponding\n",
      "most probable years. \n",
      "\n",
      "The experiment setup contains a single class WikicatEnvironment that\n",
      "implements both BaseEnvironment and BaseObjective.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import agentnet.experiments.wikicat as experiment\n",
    "print experiment.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "env = experiment.WikicatEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (85, 300) (85, 93)\n",
      "train size: (8509, 300) (8509, 93)\n",
      "features: ['Children:is_known', 'Instruments:vocals', 'Genres:electronic', 'Origin:scotland', 'Genres:jazz', 'Genres:electric', 'Occupation:bassist', 'Instruments:violin', 'Genres:instrumental', 'Occupation:entrepreneur', 'Instruments:mandolin', 'Instruments:vocalist', 'Genres:synthpop', 'decades_active:1990', 'Occupation:remixer', 'category:List_of_musical_artists_from_Japan', 'category:List_of_ambient_music_artists', 'category:List_of_crooners', 'category:List_of_country_music_performers', 'category:List_of_bass_guitarists']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "attrs, categories, feature_names = env.get_dataset()\n",
    "\n",
    "train_attrs,test_attrs,train_cats,test_cats = train_test_split(attrs,categories,test_size=0.99,random_state=32)\n",
    "\n",
    "print \"train size:\", train_attrs.shape,train_cats.shape\n",
    "print \"train size:\", test_attrs.shape,test_cats.shape\n",
    "\n",
    "print \"features:\",feature_names[::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.load_random_batch(train_attrs,train_cats,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation has to contain three parts:\n",
    " * Memory layer(s)\n",
    "  * in this case, we train two GRU layers [details below]\n",
    " * Q-values evaluation layers\n",
    "  * in this case, a lasagne dense layer based on memory layer\n",
    " * Resolver - acton picker layer\n",
    "  * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "  \n",
    "### two-layer memory architecture\n",
    "We train two memory states:\n",
    "  * first one, based on observations,\n",
    "  * second one, based on first one;\n",
    "\n",
    "Note that here we update the second memory layer based on the CURRENT state\n",
    "of the first one. Instead, you can try to feed it with a previous state.\n",
    "\n",
    "The q-values are estimated on a concatenated state, effectively on both memory\n",
    "states together, but there is no problem with limiting q-evaluator to only one:\n",
    "just pass the correct gru layer as an incoming layer to the q-evaluator.\n",
    "\n",
    "### Implementation:\n",
    "We concatenate both memories into 1 state to pass it through the session loop.\n",
    "\n",
    "To perform memory update, we need to slice the concatenated state back into\n",
    "two memory states.\n",
    "\n",
    "We do so by defining an input map function and passing it into agent.\n",
    "\n",
    "We than concatenate two new states back to form a new memory state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import ProbablisticResolver\n",
    "from agentnet.memory import GRUMemoryLayer\n",
    "from agentnet.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "n_hid_1=128 #first GRU memory\n",
    "n_hid_2=64 #second GRU memory\n",
    "\n",
    "\n",
    "_observation_layer = lasagne.layers.InputLayer([None,env.observation_size],name=\"obs_input\")\n",
    "\n",
    "_prev_gru1_layer = lasagne.layers.InputLayer([None,n_hid_1],name=\"prev_gru1_state_input\")\n",
    "_prev_gru2_layer = lasagne.layers.InputLayer([None,n_hid_2],name=\"prev_gru2_state_input\")\n",
    "\n",
    "#memory\n",
    "gru1 = GRUMemoryLayer(n_hid_1,\n",
    "                     _observation_layer,\n",
    "                     _prev_gru1_layer,\n",
    "                     name=\"gru1\")\n",
    "\n",
    "gru2 = GRUMemoryLayer(n_hid_2,\n",
    "                     gru1,        #note that it takes CURRENT gru1 output as input.\n",
    "                                  #replacing that with _prev_gru1_state would imply taking previous one.\n",
    "                     _prev_gru2_layer,\n",
    "                     name=\"gru2\")\n",
    "\n",
    "concatenated_memory = lasagne.layers.concat([gru1,gru2])\n",
    "\n",
    "#policy\n",
    "n_actions = len(feature_names)\n",
    "\n",
    "\n",
    "\n",
    "greed = theano.shared(np.float32(1),\"prob_multiplier\")\n",
    "\n",
    "policy_layer = lasagne.layers.DenseLayer(concatenated_memory, #taking both memories. \n",
    "                                                        #Replacing with gru1 or gru2 would mean taking one\n",
    "                                         num_units = n_actions,\n",
    "                                         nonlinearity=lambda x: lasagne.nonlinearities.softmax(x*greed),\n",
    "                                         name=\"policy_original\")\n",
    "\n",
    "\n",
    "state_value_layer = lasagne.layers.DenseLayer(concatenated_memory,\n",
    "                                       num_units = 1,\n",
    "                                       nonlinearity = lasagne.nonlinearities.linear,\n",
    "                                       name = \"Vpredicted\")\n",
    "\n",
    "#resolver\n",
    "\n",
    "\n",
    "resolver = ProbablisticResolver(policy_layer,assume_normalized=True,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "#we need to define the new input map because concatenated_memory is a ConcatLayer and does not have default one\n",
    "\n",
    "def custom_input_map(last_hidden,observation):\n",
    "    \"\"\"just a function that maps memory states to respective inputs\"\"\"\n",
    "    return {\n",
    "        _prev_gru1_layer:last_hidden[:,0:n_hid_1],\n",
    "        _prev_gru2_layer:last_hidden[:,n_hid_1:n_hid_1+n_hid_2],\n",
    "        _observation_layer:observation\n",
    "    }\n",
    "\n",
    "#all together\n",
    "agent = Agent(concatenated_memory,policy_layer,resolver,input_map=custom_input_map\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gru1.W_in_to_updategate,\n",
       " gru1.W_hid_to_updategate,\n",
       " gru1.b_updategate,\n",
       " gru1.W_in_to_resetgate,\n",
       " gru1.W_hid_to_resetgate,\n",
       " gru1.b_resetgate,\n",
       " gru1.W_in_to_hidden_update,\n",
       " gru1.W_hid_to_hidden_update,\n",
       " gru1.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " policy_original.W,\n",
       " policy_original.b,\n",
       " Vpredicted.W,\n",
       " Vpredicted.b]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params([resolver,state_value_layer],trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * One-step recurrent layer\n",
    "     * takes input and one's previous state\n",
    "     * returns new memory state\n",
    "   * Can be arbitrary lasagne layer\n",
    "   * Several one-step recurrent units are implemented in __agentnet.memory__\n",
    "   * Note that lasagne's default recurrent networks roll for several steps at once\n",
    "     * in other words, __using lasagne recurrent units as memory means recurrence inside recurrence__\n",
    " * Using more than one memory layer is explained in farther tutorials\n",
    "\n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#produce interaction sequences of length <= 10\n",
    "history = agent.get_sessions(env,session_length=10,\n",
    "                             batch_size=env.batch_size,\n",
    "                             additional_output_layers = [state_value_layer],\n",
    "                             )\n",
    "\n",
    "state_seq,observation_seq,hidden_seq,policy_seq,action_seq,V_seq = history\n",
    "\n",
    "\n",
    "V_seq = V_seq[:,:,0] #drop n_units dimension since we have 1 neuron only\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.get_reward_sequences(state_seq,action_seq)\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.get_whether_alive(observation_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Let us compile a function that returns all interaction logs\n",
    "get_history = theano.function([],history+(is_alive_seq,rewards_seq,),mode=theano.compile.mode.FAST_RUN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* This part is similar to the basic tutorial but for the fact that we use SARSA\n",
    " \n",
    " \n",
    " #### Get (prediction,reference) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import actor_critic\n",
    "\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.95),name = 'q_learning_gamma')\n",
    "\n",
    "\n",
    "Vreference = actor_critic.get_state_value_reference(V_seq,rewards_seq,is_alive=is_alive_seq,\n",
    "                               gamma_or_gammas=gamma,max_n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_loss = actor_critic.get_objective(policy_seq,V_seq,action_seq,Vreference,is_alive=is_alive_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.mdp import get_action_Qvalues\n",
    "action_probas = get_action_Qvalues(policy_seq,action_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, regularize_layer_params, l2\n",
    "reg_l2 = (regularize_network_params(resolver,l2) +\\\n",
    "          regularize_layer_params(state_value_layer,l2)\n",
    "         ) *1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# - H(p(s,theta)). \n",
    "\n",
    "entropy_elwise = - T.sum(policy_seq * T.log(policy_seq),axis=-1)\n",
    "entropy_reg_elwise = T.maximum(T.log(action_probas),-1e3) * entropy_elwise \n",
    "\n",
    "reg_entropy =  T.sum(entropy_reg_elwise*is_alive_seq)/ is_alive_seq.sum() * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = policy_loss + reg_l2 + reg_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = lasagne.updates.adadelta(loss,\n",
    "                                             weights,learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_session_reward = rewards_seq.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[loss,policy_loss,reg_l2,reg_entropy,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session visualization tools\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* usually useful to get insight on what worked and what din't\n",
    "* in this case, we print strings following pattern\n",
    "  * [action_name] ([predicted action qvalue]) -> reward [reference qvalue] | next iteration\n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are Qvalues for actions\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "__Warning! the visualization tools are underdeveloped and only allow simple operations.__\n",
    "\n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        hidden_seq,policy_seq, action_seq,rewards_seq,policy_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False):\n",
    "        \n",
    "    hidden_log,qvalues_log,actions_log,reward_log, reference_qv_log, is_alive_log = get_printables()\n",
    "    \n",
    "    \n",
    "    print_sessions(qvalues_log,actions_log,reward_log,\n",
    "                   is_alive_seq = is_alive_log,\n",
    "                   #hidden_seq=hidden_log, #do not plot hidden since there's too many actions already\n",
    "                   reference_qvalues_seq = reference_qv_log,\n",
    "                   action_names=feature_names,\n",
    "                   legend = False, #do not show legend since there's too many labeled objects\n",
    "                  plot_qvalues = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "env.load_random_batch(train_attrs,train_cats,1)\n",
    "display_sessions(with_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tools for model persistence\n",
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = -7.\n",
    "ma_reward_greedy = -7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 250000\n",
    "batch_size= 50\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    #train\n",
    "    env.load_random_batch(train_attrs,train_cats,batch_size)\n",
    "    \n",
    "\n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #update resolver's greed\n",
    "    if epoch_counter%10 ==0:\n",
    "        current_greed =  0.05 + 0.95*( np.exp(epoch_counter/(epoch_counter+5000.))/np.e)\n",
    "        greed.set_value(np.float32(current_greed))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty,entropy_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        greed.set_value(np.float32(3))\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected reward greed=3\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to less greedy\n",
    "        greed.set_value(np.float32(current_greed))\n",
    "\n",
    "        print \"epoch %i,loss %.5f, greed %.5f, rewards: ( in_train %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_greed,ma_reward_current,ma_reward_greedy)\n",
    "        print \"rec %.3f l2 %.3f ent %.3f\"%(q_loss,l2_penalty,entropy_penalty)\n",
    "\n",
    "    if epoch_counter %1000 ==0:\n",
    "        print \"Learning curves:\"\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print \"Random session examples\"\n",
    "        env.load_random_batch(train_attrs,train_cats,3)\n",
    "        display_sessions(with_plots=False)\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %10000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print \"saved\", snap_name\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random session examples\"\n",
    "env.load_random_batch(train_attrs,train_cats,10)\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
