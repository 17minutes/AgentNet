{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment name and snapshot folder (used for model persistence)\n",
    "experiment_setup_name = \"research.wikicat.sessions\"\n",
    "snapshot_path = \"/home/jheuristic/yozhik/agentnet_snapshots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=gpu2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n",
      "/home/jheuristic/thenv/local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "#the problem is too simple to be run on GPU. Seriously.\n",
    "%env THEANO_FLAGS='device=gpu2'\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "import lasagne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial vuilds above the basic tutorial and shows several advanced tools\n",
    "* multi-layer (and in principle, arbitrary) agent memory\n",
    "* different reinforcement learning algorithms\n",
    "* model persistence\n",
    "\n",
    "__[todo: add more]__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we load an experiment environment (description below)\n",
    "* Designing one from scratch is explained in later tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This experiment Wikipedia data sample on musicians, scraped via the scripts present in this directory. \n",
      "For each musician, we know a number of boolean features (factors) on various topics like \n",
      " * whether or not he/she was active in 1990's, 2000's, etc.\n",
      " * whether or not he/she plays guitar, piano, etc\n",
      " * whether or not he/she was born in 50's, 60's, etc.\n",
      " * what wikipedia categories does he/she belong to\n",
      " etc.\n",
      "\n",
      "\n",
      "\n",
      "In the initial moment of time, agent knows nothing about any of them.\n",
      "At each turn, agent may decide to\n",
      " - \"open\" one of the hidden factors.\n",
      "   - if the factor turns out to be 1, agent receives +3 reward for Wikipedia categories, +1 for other categories,\n",
      "   - Otherwise, the reward equals -1 for Wikipedia categories, -1 for other categores\n",
      "   - all these rewards are parameterisable during environment creation\n",
      " - decide to quit session\n",
      "   - yields reward of 0 and ends the interaction.\n",
      "   - all farther actions will have no effect until next session\n",
      "\n",
      "It is expected, that in order to maximize it's expected reward, the agent\n",
      "will converge to a strategy of polling several key features and then utilizing learned\n",
      "dependencies between these factors and other ones. For example, if a particular genre was\n",
      "popular during particular decays, it makes sense to poll genres and than \"open\" the corresponding\n",
      "most probable years. \n",
      "\n",
      "The experiment setup contains a single class WikicatEnvironment that\n",
      "implements both BaseEnvironment and BaseObjective.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import agentnet.experiments.wikicat as experiment\n",
    "print experiment.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "env = experiment.WikicatEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (85, 300) (85, 93)\n",
      "train size: (8509, 300) (8509, 93)\n",
      "features: ['Children:is_known', 'Instruments:vocals', 'Genres:electronic', 'Origin:scotland', 'Genres:jazz', 'Genres:electric', 'Occupation:bassist', 'Instruments:violin', 'Genres:instrumental', 'Occupation:entrepreneur', 'Instruments:mandolin', 'Instruments:vocalist', 'Genres:synthpop', 'decades_active:1990', 'Occupation:remixer', 'category:List_of_musical_artists_from_Japan', 'category:List_of_ambient_music_artists', 'category:List_of_crooners', 'category:List_of_country_music_performers', 'category:List_of_bass_guitarists']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "attrs, categories, feature_names = env.get_dataset()\n",
    "\n",
    "train_attrs,test_attrs,train_cats,test_cats = train_test_split(attrs,categories,test_size=0.99,random_state=32)\n",
    "\n",
    "print \"train size:\", train_attrs.shape,train_cats.shape\n",
    "print \"train size:\", test_attrs.shape,test_cats.shape\n",
    "\n",
    "print \"features:\",feature_names[::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.load_random_batch(train_attrs,train_cats,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup\n",
    "* An agent implementation has to contain three parts:\n",
    " * Memory layer(s)\n",
    "  * in this case, we train two GRU layers [details below]\n",
    " * Q-values evaluation layers\n",
    "  * in this case, a lasagne dense layer based on memory layer\n",
    " * Resolver - acton picker layer\n",
    "  * in this case, the resolver has epsilon-greedy policy\n",
    "  \n",
    "  \n",
    "### two-layer memory architecture\n",
    "We train two memory states:\n",
    "  * first one, based on observations,\n",
    "  * second one, based on first one;\n",
    "\n",
    "Note that here we update the second memory layer based on the CURRENT state\n",
    "of the first one. Instead, you can try to feed it with a previous state.\n",
    "\n",
    "The q-values are estimated on a concatenated state, effectively on both memory\n",
    "states together, but there is no problem with limiting q-evaluator to only one:\n",
    "just pass the correct gru layer as an incoming layer to the q-evaluator.\n",
    "\n",
    "### Implementation:\n",
    "We concatenate both memories into 1 state to pass it through the session loop.\n",
    "\n",
    "To perform memory update, we need to slice the concatenated state back into\n",
    "two memory states.\n",
    "\n",
    "We do so by defining an input map function and passing it into agent.\n",
    "\n",
    "We than concatenate two new states back to form a new memory state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "from agentnet.memory import GRUMemoryLayer\n",
    "from agentnet.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "n_hid_1=512 #first GRU memory\n",
    "n_hid_2=256 #second GRU memory\n",
    "n_hid_3=256 #third GRU memory\n",
    "\n",
    "\n",
    "_observation_layer = lasagne.layers.InputLayer([None,env.observation_size],name=\"obs_input\")\n",
    "\n",
    "_prev_gru1_layer = lasagne.layers.InputLayer([None,n_hid_1],name=\"prev_gru1_state_input\")\n",
    "_prev_gru2_layer = lasagne.layers.InputLayer([None,n_hid_2],name=\"prev_gru2_state_input\")\n",
    "_prev_gru3_layer = lasagne.layers.InputLayer([None,n_hid_3],name=\"prev_gru2_state_input\")\n",
    "\n",
    "#memory\n",
    "gru1 = GRUMemoryLayer(n_hid_1,\n",
    "                     _observation_layer,\n",
    "                     _prev_gru1_layer,\n",
    "                     name=\"gru1\")\n",
    "\n",
    "gru2 = GRUMemoryLayer(n_hid_2,\n",
    "                     gru1,        #note that it takes CURRENT gru1 output as input.\n",
    "                                  #replacing that with _prev_gru1_state would imply taking previous one.\n",
    "                     _prev_gru2_layer,\n",
    "                     name=\"gru2\")\n",
    "\n",
    "gru3 = GRUMemoryLayer(n_hid_3,\n",
    "                     gru2,        #note that it takes CURRENT gru1 output as input.\n",
    "                                  #replacing that with _prev_gru1_state would imply taking previous one.\n",
    "                     _prev_gru3_layer,\n",
    "                     name=\"gru2\")\n",
    "\n",
    "\n",
    "concatenated_memory = lasagne.layers.concat([gru1,gru2,gru3])\n",
    "\n",
    "#q_eval\n",
    "n_actions = len(feature_names)\n",
    "q_eval = lasagne.layers.DenseLayer(concatenated_memory, #taking both memories. \n",
    "                                                        #Replacing with gru1 or gru2 would mean taking one\n",
    "                                   num_units = n_actions,\n",
    "                                   nonlinearity=lasagne.nonlinearities.linear,name=\"QEvaluator\")\n",
    "#resolver\n",
    "epsilon = theano.shared(np.float32(0.9),\"e-greedy.epsilon\")\n",
    "\n",
    "resolver = EpsilonGreedyResolver(q_eval,epsilon=epsilon,name=\"resolver\")\n",
    "\n",
    "\n",
    "\n",
    "#we need to define the new input map because concatenated_memory is a ConcatLayer and does not have default one\n",
    "\n",
    "def custom_input_map(last_hidden,observation):\n",
    "    \"\"\"just a function that maps memory states to respective inputs\"\"\"\n",
    "    return {\n",
    "        _prev_gru1_layer:last_hidden[:,0:n_hid_1],\n",
    "        _prev_gru2_layer:last_hidden[:,n_hid_1:n_hid_1+n_hid_2],\n",
    "        _prev_gru3_layer:last_hidden[:,n_hid_1+n_hid_2:],\n",
    "        _observation_layer:observation\n",
    "    }\n",
    "\n",
    "#all together\n",
    "agent = Agent(concatenated_memory,q_eval,resolver,input_map=custom_input_map\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gru1.W_in_to_updategate,\n",
       " gru1.W_hid_to_updategate,\n",
       " gru1.b_updategate,\n",
       " gru1.W_in_to_resetgate,\n",
       " gru1.W_hid_to_resetgate,\n",
       " gru1.b_resetgate,\n",
       " gru1.W_in_to_hidden_update,\n",
       " gru1.W_hid_to_hidden_update,\n",
       " gru1.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " gru2.W_in_to_updategate,\n",
       " gru2.W_hid_to_updategate,\n",
       " gru2.b_updategate,\n",
       " gru2.W_in_to_resetgate,\n",
       " gru2.W_hid_to_resetgate,\n",
       " gru2.b_resetgate,\n",
       " gru2.W_in_to_hidden_update,\n",
       " gru2.W_hid_to_hidden_update,\n",
       " gru2.b_hidden_update,\n",
       " QEvaluator.W,\n",
       " QEvaluator.b]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent setup in detail\n",
    "* __Memory layers__\n",
    " * One-step recurrent layer\n",
    "     * takes input and one's previous state\n",
    "     * returns new memory state\n",
    "   * Can be arbitrary lasagne layer\n",
    "   * Several one-step recurrent units are implemented in __agentnet.memory__\n",
    "   * Note that lasagne's default recurrent networks roll for several steps at once\n",
    "     * in other words, __using lasagne recurrent units as memory means recurrence inside recurrence__\n",
    " * Using more than one memory layer is explained in farther tutorials\n",
    "\n",
    "\n",
    "* __Q-values evaluation layer__\n",
    " * Can be arbitrary lasagne network\n",
    " * returns predicted Q-values for each action\n",
    " * Usually depends on memory as an input\n",
    "\n",
    "\n",
    "* __Resolver__ - action picker\n",
    " * Decides on what action is taken\n",
    " * Normally takes Q-values as input\n",
    " * Currently all experiments require integer output\n",
    " * Several resolver layers are implemented in __agentnet.resolver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* an agent has a method that produces symbolic environment interaction sessions\n",
    "* interactions result in sequences of observations, actions, q-values,etc\n",
    "* one has to pre-define maximum session length.\n",
    " * in this case, environment implements an indicator of whether session has ended by current tick\n",
    "* Since this environment also implements Objective methods, it can evaluate rewards for each [batch, time_tick]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#produce interaction sequences of length <= 10\n",
    "history = agent.get_sessions(env,session_length=10,\n",
    "                       batch_size=env.batch_size,)\n",
    "\n",
    "state_seq,observation_seq,hidden_seq,qvalues_seq,action_seq = history\n",
    "\n",
    "#get rewards for all actions\n",
    "rewards_seq = env.get_reward_sequences(state_seq,action_seq)\n",
    "\n",
    "#get indicator whether session is still active\n",
    "is_alive_seq = env.get_whether_alive(observation_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Let us compile a function that returns all interaction logs\n",
    "get_history = theano.function([],history+(is_alive_seq,rewards_seq,),mode=theano.compile.mode.FAST_RUN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [dev] Session pool init & update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.environment import SessionPoolEnvironment,SessionBatchEnvironment\n",
    "\n",
    "session_pool = SessionPoolEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a new batch was loaded into MAIN environment\n",
    "batch_size_interactive = 250\n",
    "batch_size_replay = 100\n",
    "pool_size = 1000\n",
    "seq_length = 10\n",
    "\n",
    "\n",
    "env.load_random_batch(train_attrs,train_cats,pool_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the pool\n",
    "initial_update_pool= session_pool.get_session_updates(observation_seq,action_seq,rewards_seq)\n",
    "\n",
    "\n",
    "init_pool_fun = theano.function([],session_pool.pool_size,updates=initial_update_pool)\n",
    "init_pool_fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add new sessions to pool\n",
    "\n",
    "new_observation_seq = T.concatenate([session_pool.observations,observation_seq],axis=0)\n",
    "new_action_seq = T.concatenate([session_pool.actions,action_seq],axis=0)\n",
    "new_rewards_seq = T.concatenate([session_pool.rewards,rewards_seq],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#expand and shrink in one function\n",
    "\n",
    "\n",
    "#temp environment that replayhs sessions of the expanded pool\n",
    "expanded_pool = SessionBatchEnvironment(new_observation_seq,new_action_seq,new_rewards_seq)\n",
    "\n",
    "\n",
    "replay_log = agent.get_sessions(expanded_pool,session_length=seq_length,\n",
    "                       batch_size=expanded_pool.actions.shape[0],)\n",
    "\n",
    "\n",
    "_,_,hidden_replay,qvalues_replay,untaken_actions_replay = replay_log\n",
    "actions_replay = expanded_pool.actions\n",
    "#in these sessions, states are fake single-zero placeholders, \n",
    "#observations are equal to session_pool.observations,\n",
    "#untaken_actions_replay are actions that agent would want to take, but didn't\n",
    "#session_pool.actions are the replayed actions\n",
    "\n",
    "\n",
    "#Since actions did not change, we can use the old rewards\n",
    "#TODO: add DontDoThatError to SessionEnvironment.get_reward_whatever\n",
    "rewards_replay = expanded_pool.rewards\n",
    "\n",
    "#indicator that i-th batch is alive by j-th timestamp\n",
    "is_alive_replay = env.get_whether_alive(expanded_pool.observations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reference replay Qvalues\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "gamma = theano.shared(np.float32(0.95),name = 'q_learning_gamma')\n",
    "\n",
    "Qreference_replay = qlearning.get_reference(qvalues_replay,\n",
    "                               actions_replay,\n",
    "                               rewards_replay,\n",
    "                               gamma_or_gammas=gamma,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zero-out future rewards at session end\n",
    "from agentnet.utils.mdp import get_end_indicator\n",
    "\n",
    "is_end_replay = get_end_indicator(is_alive_replay).nonzero()\n",
    "\n",
    "# \"set reference Qvalues at end action ids to just the immediate rewards\"\n",
    "Qreference_replay = T.set_subtensor(Qreference_replay[is_end_replay],\n",
    "                                    rewards_replay[is_end_replay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prevent gradient updates over reference Qvalues (since they depend on predicted Qvalues)\n",
    "from agentnet.utils import consider_constant\n",
    "Qreference_replay = consider_constant(Qreference_replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.mdp import get_action_Qvalues\n",
    "action_Qvalues_replay = get_action_Qvalues(qvalues_replay,actions_replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Qvalues error\n",
    "\n",
    "squared_error = lasagne.objectives.squared_error(action_Qvalues_replay,Qreference_replay)\n",
    "\n",
    "#session mse = sum(errors_while_alive) / num_alive_ticks\n",
    "replay_session_mse =  (squared_error * is_alive_replay).sum(axis=1) / T.sum(is_alive_replay,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_by_error = T.argsort(replay_session_mse)\n",
    "\n",
    "survived_ids = order_by_error[-pool_size:][::-1]\n",
    "\n",
    "shrinked_observation_replay = expanded_pool.observations[survived_ids] \n",
    "shrinked_action_replay = expanded_pool.actions[survived_ids]\n",
    "shrinked_rewards_replay = expanded_pool.rewards[survived_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#metrics\n",
    "shrinked_pool_mse = replay_session_mse[survived_ids].mean()\n",
    "avg_new_reward = T.sum(rewards_seq,axis=1).mean()\n",
    "avg_pool_reward = T.sum(rewards_replay,axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "pool_updates = session_pool.get_session_updates(shrinked_observation_replay,shrinked_action_replay,shrinked_rewards_replay)\n",
    "\n",
    "update_pool_fun = theano.function([],[shrinked_pool_mse,avg_new_reward,avg_pool_reward],updates=pool_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay training function\n",
    "* training network on a small random batch of replays\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get random sample of replay session pool [new sample each iteration]\n",
    "\n",
    "session_batch = session_pool.sample_session_batch(max_n_samples=batch_size_replay,replace=True)\n",
    "\n",
    "\n",
    "#get experience replay sessions\n",
    "replay_batch = agent.get_sessions(session_batch,session_length=seq_length,\n",
    "                       batch_size=session_batch.actions.shape[0],)\n",
    "\n",
    "\n",
    "#just like when updating pool\n",
    "_,_,hidden_batch,qvalues_batch,untaken_actions_batch = replay_batch\n",
    "actions_batch = session_batch.actions\n",
    "\n",
    "\n",
    "#Since actions did not change, we can use the old rewards\n",
    "rewards_batch = session_batch.rewards\n",
    "\n",
    "#indicator that i-th batch is alive by j-th timestamp\n",
    "is_alive_batch = env.get_whether_alive(session_batch.observations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get reference and action qvalues\n",
    "\n",
    "Qreference_batch = qlearning.get_reference(qvalues_batch,\n",
    "                               actions_batch,\n",
    "                               rewards_batch,\n",
    "                               gamma_or_gammas=gamma,)\n",
    "\n",
    "is_end_batch = get_end_indicator(is_alive_batch).nonzero()\n",
    "\n",
    "# \"set reference Qvalues at end action ids to just the immediate rewards\"\n",
    "Qreference_batch = T.set_subtensor(Qreference_batch[is_end_batch],\n",
    "                                    rewards_batch[is_end_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.mdp import get_action_Qvalues\n",
    "action_Qvalues_batch = get_action_Qvalues(qvalues_batch,actions_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tensor of elementwise squared errors\n",
    "batch_mse = lasagne.objectives.squared_error(Qreference_batch,action_Qvalues_batch)\n",
    "\n",
    "#zero-out ticks after session ended\n",
    "batch_mse *= is_alive_batch\n",
    "\n",
    "#compute average of squared error sums per session\n",
    "batch_mse_loss = batch_mse.sum() / is_alive_batch.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_loss = batch_mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = lasagne.updates.adadelta(batch_loss,weights,learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some auxilary evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_batch_reward = rewards_batch.sum(axis=1).mean()\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([],[batch_loss,mean_batch_reward],updates=updates)\n",
    "\n",
    "evaluation_fun = theano.function([],[batch_loss,batch_mse_loss,reg_l2,mean_batch_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session visualization tools\n",
    "\n",
    "\n",
    "* this is a completely optional step of visualizing agent's sessions as chains of actions\n",
    "* usually useful to get insight on what worked and what din't\n",
    "* in this case, we print strings following pattern\n",
    "  * [action_name] ([predicted action qvalue]) -> reward [reference qvalue] | next iteration\n",
    "\n",
    "* plot shows\n",
    "    * time ticks over X, abstract values over Y\n",
    "    * bold lines are Qvalues for actions\n",
    "    * dots on bold lines represent what actions were taken at each moment of time\n",
    "    * dashed lines are agent's hidden state neurons\n",
    "    * blue vertical line - session end\n",
    "    \n",
    "    \n",
    "__Warning! the visualization tools are underdeveloped and only allow simple operations.__\n",
    "\n",
    "if you found yourself struggling to make it do what you want for 5 minutes, go write your own tool [and contribute it :)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reference_Qvalues_seq = qlearning.get_reference(qvalues_seq,\n",
    "                               action_seq,\n",
    "                               rewards_seq,\n",
    "                               gamma_or_gammas=gamma,)\n",
    "\n",
    "is_end_seq = get_end_indicator(is_alive_seq).nonzero()\n",
    "\n",
    "# \"set reference Qvalues at end action ids to just the immediate rewards\"\n",
    "reference_Qvalues_seq = T.set_subtensor(reference_Qvalues_seq[is_end_seq],\n",
    "                                    rewards_seq[is_end_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.display.sessions import print_sessions\n",
    "get_printables = theano.function([], [\n",
    "        hidden_seq,qvalues_seq, action_seq,rewards_seq,reference_Qvalues_seq,is_alive_seq\n",
    "    ])\n",
    "\n",
    "def display_sessions(with_plots = False):\n",
    "        \n",
    "    hidden_log,qvalues_log,actions_log,reward_log, reference_qv_log, is_alive_log = get_printables()\n",
    "    \n",
    "    \n",
    "    print_sessions(qvalues_log,actions_log,reward_log,\n",
    "                   is_alive_seq = is_alive_log,\n",
    "                   #hidden_seq=hidden_log, #do not plot hidden since there's too many actions already\n",
    "                   reference_qvalues_seq = reference_qv_log,\n",
    "                   action_names=feature_names,\n",
    "                   legend = False, #do not show legend since there's too many labeled objects\n",
    "                  plot_qvalues = with_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "env.load_random_batch(train_attrs,train_cats,1)\n",
    "display_sessions(with_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tools for model persistence\n",
    "from agentnet.utils.persistence import save,load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = -7.\n",
    "ma_reward_greedy = -7.\n",
    "ma_reward_pool = -7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000000\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    #train\n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    #update pool\n",
    "    if epoch_counter %10==0:\n",
    "\n",
    "        \n",
    "        #reward loss moving average\n",
    "        env.load_random_batch(train_attrs,train_cats,batch_size_interactive)\n",
    "        _,avg_reward,avg_pool_reward = update_pool_fun()\n",
    "        ma_reward_pool = alpha*avg_pool_reward+ (1-alpha)*ma_reward_pool\n",
    "        score_log[\"session pool e-greedy reward\"][epoch_counter] = ma_reward_pool\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%10 ==0:\n",
    "        current_epsilon =  0.05 + 0.95*np.exp(-epoch_counter/10000.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "\n",
    "        print \"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy)\n",
    "        print \"rec %.3f reg %.3f\"%(q_loss,l2_penalty)\n",
    "\n",
    "    if epoch_counter %1000 ==0:\n",
    "        print \"Learning curves:\"\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print \"Random session examples\"\n",
    "        env.load_random_batch(train_attrs,train_cats,3)\n",
    "        display_sessions(with_plots=False)\n",
    "    \n",
    "    #save snapshot\n",
    "    if epoch_counter %10000 ==0:\n",
    "        snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), epoch_counter)\n",
    "        save(resolver,snap_name)\n",
    "        print \"saved\", snap_name\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Random session examples\"\n",
    "env.load_random_batch(train_attrs,train_cats,10)\n",
    "display_sessions(with_plots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load earlier snapshot\n",
    "snap_name = \"{}.epoch{}.pcl\".format(os.path.join(snapshot_path,experiment_setup_name), 10000)\n",
    "load(resolver,snap_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Earlier session examples\"\n",
    "env.load_random_batch(train_attrs,train_cats,3)\n",
    "display_sessions(with_plots=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
